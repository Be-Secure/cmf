{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMF in a nutshell \u00b6 CMF (Common Metadata Framework) collects and stores information associated with Machine Learning (ML) pipelines. It also implements APIs to query this metadata. The CMF adopts a data-first approach: all artifacts (such as datasets, ML models and performance metrics) recorded by the framework are versioned and identified by their content hash. Installation \u00b6 CMF requires 3.7 >= Python <= 3.9. Create python virtual environment: Conda VirtualEnv conda create -n cmf python = 3 .8 conda activate cmf virtualenv --python = 3 .8 .cmf source .cmf/bin/activate Install CMF Latest version form GitHub Stable version form PyPI pip install https://github.com/HewlettPackard/cmf # Work in progress: not available yet. # pip install cmflib Jupyter Lab docker container with CMF pre-installed \u00b6 Introduction \u00b6 Complex ML projects rely on ML pipelines to train and test ML models. An ML pipeline is a sequence of stages where each stage performs a particular task, such as data loading and pre-processing, ML model training and testing. Stages consume inputs and produce outputs . are parametrized by parameters that guide the process of producing outputs. CMF uses abstractions defined in ML Metadata (MLMD) library to represent CMF concepts. Each pipeline has a name. Users provide it when they initialize the CMF. Each stage is characterized by metadata represented as MLMD's Context object. When users actually run a stage, this is recorded by the MLMD's Execution object. Inputs and outputs of stages are represented as MLMD's Artifact object, while parameters of stages are recorded as properties of executions. 1 Init 2 Stage type 3 New execution 4 Log Artifacts Start tracking the pipeline metadata by initializing the CMF runtime. The metadata will be associated with the pipeline named test_pipeline . from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" , ) Before we can start tracking metadata, we need to let CMF know about stage type. This is not yet associated with this particular execution. context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"train\" ) Now we can create a new stage execution associated with the train stage. The CMF always creates a new execution, and will adjust its name, so it's unique. This is also the place where we log stage parameters . execution : mlmd . proto . Execution = cmf . create_execution ( execution_type = \"train\" , custom_properties = { \"num_epochs\" : 100 , \"learning_rate\" : 0.01 } ) Finally, we can log an input (train dataset), and once trained, an output (ML model) artifacts. cmf . log_dataset ( 'artifacts/test_dataset.csv' , # Dataset path \"input\" # This is INPUT artifact ) cmf . log_model ( \"artifacts/model.pkl\" , # Model path event = \"output\" # This is OUTPUT artifact ) Quick Example \u00b6 Simple \"getting started\" example is described here . API Overview \u00b6 Import CMF . from cmflib import cmf Create the metadata writer . The metadata writer is responsible for managing a CMF backend to record the pipeline metadata. Internally, it creates a pipeline abstraction that groups individual stages and their executions. All stages, their executions and produced artifacts will be associated with a pipeline with the given name. cmf = cmf . Cmf ( filename = \"mlmd\" , # Path to ML Metadata file. pipeline_name = \"mnist\" # Name of a ML pipeline. ) Define a stage . An ML pipeline can have multiple stages, and each stage can be associated with multiple executions. A stage is like a class in the world of object-oriented programming languages. A context (stage description) defines what this stage looks like (name and optional properties), and is created with the create_context method. context = cmf . create_context ( pipeline_stage = \"download\" , # Stage name custom_properties = { # Optional properties \"uses_network\" : True , # Downloads from the Internet \"disk_space\" : \"10GB\" # Needs this much space } ) Create a stage execution . A stage in ML pipeline can have multiple executions. Every run is marked as an execution. This API helps to track the metadata associated with the execution, like stage parameters (e.g., number of epochs and learning rate for train stages). The stage execution name does not need to be the same as the name of its context. Moreover, the CMF will adjust this name to ensure every execution has a unique name. The CMF will internally associate this execution with the context created previously. Stage executions are created by calling the create_execution method. execution = cmf . create_execution ( execution_type = \"download\" , # Execution name. custom_properties = { # Execution parameters \"url\" : \"https://a.com/mnist.gz\" # Data URL. } ) Log artifacts . A stage execution can consume (inputs) and produce (outputs) multiple artifacts (datasets, models and performance metrics). The path of these artifacts must be relative to the project (repository) root path. Artifacts might have optional metadata associated with them. These metadata could include feature statistics for ML datasets, or useful parameters for ML models (such as, for instance, number of trees in a random forest classifier). Datasets are logged with the log_dataset method. cmf . log_dataset ( 'data/mnist.gz' , \"input\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : 'raw' }) cmf . log_dataset ( 'data/train.csv' , \"output\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : \"train_split\" }) cmf . log_dataset ( 'data/test.csv' , \"output\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : \"test_split\" }) ML models produced by training stages are logged using log_model API. ML models can be both input and output artifacts. The metadata associated with the artifact could be logged as an optional argument. # In train stage cmf . log_model ( path = \"model/rf.pkl\" , event = \"output\" , model_framework = \"scikit-learn\" , model_type = \"RandomForestClassifier\" , model_name = \"RandomForestClassifier:default\" ) # In test stage cmf . log_model ( path = \"model/rf.pkl\" , event = \"input\" ) Metrics of every optimization step (one epoch of Stochastic Gradient Descent, or one boosting round in Gradient Boosting Trees) are logged using log_metric API. #Can be called at every epoch or every step in the training. This is logged to a parquet file and committed at the # commit stage. #Inside training loop while True : cmf . log_metric ( \"training_metrics\" , { \"loss\" : loss }) cmf . commit_metrics ( \"training_metrics\" ) Stage metrics , or final metrics, are logged with the log_execution_metrics method. These are final metrics of a stage, such as final train or test accuracy. cmf . log_execution_metrics ( \"metrics\" , { \"avg_prec\" : avg_prec , \"roc_auc\" : roc_auc }) Dataslices are intended to be used to track subsets of the data. For instance, this can be used to track and compare accuracies of ML models on these subsets to identify model bias. Data slices are created with the create_dataslice method. dataslice = cmf . create_dataslice ( \"slice-a\" ) for i in range ( 1 , 20 , 1 ): j = random . randrange ( 100 ) dataslice . add_data ( \"data/raw_data/\" + str ( j ) + \".xml\" ) dataslice . commit () Graph Layer Overview \u00b6 CMF library has an optional graph layer which stores the relationships in a Neo4J graph database. To use the graph layer, the graph parameter in the library init call must be set to true (it is set to false by default). The library reads the configuration parameters of the graph database from the following environment variables: NEO4J_URI , NEO4J_USER_NAME and NEO4J_PASSWD . They need to be made available in a user environment, e.g.: export NEO4J_URI = \"bolt://10.93.244.219:7687\" export NEO4J_USER_NAME = neo4j export NEO4J_PASSWD = neo4j To use the graph layer, instantiate the CMF with graph=True parameter: from cmflib import cmf cmf = cmf . Cmf ( filename = \"mlmd\" , pipeline_name = \"anomaly_detection_pipeline\" , graph = True ) Use a Jupyterlab Docker environment with CMF pre-installed \u00b6 CMF has a docker-compose file which creates two docker containers, - JupyterLab Notebook Environment with CMF pre installed. - Accessible at http://[HOST.IP.AD.DR]:8888 (default token: docker ) - Within the Jupyterlab environment, a startup script switches context to $USER:$GROUP as specified in .env - example-get-started from this repo is bind mounted into /home/jovyan/example-get-started - Neo4j Docker container to store and access lineages. Step 1. \u00b6 create .env file in current folder using env-example as a template. Modify the .env file for the following variables USER,UID,GROUP,GID,GIT_USER_NAME,GIT_USER_EMAIL,GIT_REMOTE_URL #These are used by docker-compose.yml Step 2. \u00b6 Update docker-compose.yml as needed. your .ssh folder is mounted inside the docker conatiner to enable you to push and pull code from git To-Do Create these directories in your home folder mkdir $HOME/workspace mkdir $HOME/dvc_remote workspace - workspace will be mounted inside the cmf pre-installed docker conatiner (can be your code directory) dvc_remote - remote data store for dvc or Change the below lines in docker-compose to reflect the appropriate directories If your workspace is named \"experiment\" change the below line $HOME/workspace:/home/jovyan/workspace to $HOME/experiment:/home/jovyan/wokspace If your remote is /extmount/data change the line $HOME/dvc_remote:/home/jovyan/dvc_remote to /extmount/data:/home/jovyan/dvc_remote Start the docker docker-compose up --build -d Access the jupyter notebook http://[HOST.IP.AD.DR]:8888 (default token: docker ) Click the terminal icon Quick Start cd example-get-started sh initialize.sh sh test_script.sh dvc push The above steps will run a pre coded example pipeline and the metadata is stored in a file named \"mlmd\". The artifacts created will be pushed to configured dvc remote (default: /home/dvc_remote) The stored metadata is displayed as Metadata lineage can be accessed in neo4j. Open http://host:7475/browser/ Connect to server with default password neo4j123 (To change this modify .env file) Run the query MATCH (a:Execution)-[r]-(b) WHERE (b:Dataset or b:Model or b:Metrics) RETURN a,r, b Expected output Jupyter Lab Notebook Select the kernel as Python[conda env:python37] Shutdown/remove (Remove volumes as well) docker-compose down -v License \u00b6 CMF is an open source project hosted on GitHub and distributed according to the Apache 2.0 licence . We are welcome user contributions - send us a message on the Slack channel or open a GitHub issue or a pull request on GitHub. Citation \u00b6 @mist { foltin2022cmf , title = {Self-Learning Data Foundation for Scientific AI} , author = {Martin Foltin, Annmary Justine, Sergey Serebryakov, Cong Xu, Aalap Tripathy, Suparna Bhattacharya, Paolo Faraboschi} , year = {2022} , note = {Presented at the \"Monterey Data Conference\"} , URL = {https://drive.google.com/file/d/1Oqs0AN0RsAjt_y9ZjzYOmBxI8H0yqSpB/view} , } Community \u00b6 Help Common Metadata Framework and its documentation are in active stage of development and are very new. If there is anything unclear, missing or there's a typo, please, open an issue or pull request on GitHub .","title":"CMF in a nutshell"},{"location":"#cmf-in-a-nutshell","text":"CMF (Common Metadata Framework) collects and stores information associated with Machine Learning (ML) pipelines. It also implements APIs to query this metadata. The CMF adopts a data-first approach: all artifacts (such as datasets, ML models and performance metrics) recorded by the framework are versioned and identified by their content hash.","title":"CMF in a nutshell"},{"location":"#installation","text":"CMF requires 3.7 >= Python <= 3.9. Create python virtual environment: Conda VirtualEnv conda create -n cmf python = 3 .8 conda activate cmf virtualenv --python = 3 .8 .cmf source .cmf/bin/activate Install CMF Latest version form GitHub Stable version form PyPI pip install https://github.com/HewlettPackard/cmf # Work in progress: not available yet. # pip install cmflib","title":"Installation"},{"location":"#jupyter-lab-docker-container-with-cmf-pre-installed","text":"","title":"Jupyter Lab docker container with CMF pre-installed"},{"location":"#introduction","text":"Complex ML projects rely on ML pipelines to train and test ML models. An ML pipeline is a sequence of stages where each stage performs a particular task, such as data loading and pre-processing, ML model training and testing. Stages consume inputs and produce outputs . are parametrized by parameters that guide the process of producing outputs. CMF uses abstractions defined in ML Metadata (MLMD) library to represent CMF concepts. Each pipeline has a name. Users provide it when they initialize the CMF. Each stage is characterized by metadata represented as MLMD's Context object. When users actually run a stage, this is recorded by the MLMD's Execution object. Inputs and outputs of stages are represented as MLMD's Artifact object, while parameters of stages are recorded as properties of executions. 1 Init 2 Stage type 3 New execution 4 Log Artifacts Start tracking the pipeline metadata by initializing the CMF runtime. The metadata will be associated with the pipeline named test_pipeline . from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" , ) Before we can start tracking metadata, we need to let CMF know about stage type. This is not yet associated with this particular execution. context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"train\" ) Now we can create a new stage execution associated with the train stage. The CMF always creates a new execution, and will adjust its name, so it's unique. This is also the place where we log stage parameters . execution : mlmd . proto . Execution = cmf . create_execution ( execution_type = \"train\" , custom_properties = { \"num_epochs\" : 100 , \"learning_rate\" : 0.01 } ) Finally, we can log an input (train dataset), and once trained, an output (ML model) artifacts. cmf . log_dataset ( 'artifacts/test_dataset.csv' , # Dataset path \"input\" # This is INPUT artifact ) cmf . log_model ( \"artifacts/model.pkl\" , # Model path event = \"output\" # This is OUTPUT artifact )","title":"Introduction"},{"location":"#quick-example","text":"Simple \"getting started\" example is described here .","title":"Quick Example"},{"location":"#api-overview","text":"Import CMF . from cmflib import cmf Create the metadata writer . The metadata writer is responsible for managing a CMF backend to record the pipeline metadata. Internally, it creates a pipeline abstraction that groups individual stages and their executions. All stages, their executions and produced artifacts will be associated with a pipeline with the given name. cmf = cmf . Cmf ( filename = \"mlmd\" , # Path to ML Metadata file. pipeline_name = \"mnist\" # Name of a ML pipeline. ) Define a stage . An ML pipeline can have multiple stages, and each stage can be associated with multiple executions. A stage is like a class in the world of object-oriented programming languages. A context (stage description) defines what this stage looks like (name and optional properties), and is created with the create_context method. context = cmf . create_context ( pipeline_stage = \"download\" , # Stage name custom_properties = { # Optional properties \"uses_network\" : True , # Downloads from the Internet \"disk_space\" : \"10GB\" # Needs this much space } ) Create a stage execution . A stage in ML pipeline can have multiple executions. Every run is marked as an execution. This API helps to track the metadata associated with the execution, like stage parameters (e.g., number of epochs and learning rate for train stages). The stage execution name does not need to be the same as the name of its context. Moreover, the CMF will adjust this name to ensure every execution has a unique name. The CMF will internally associate this execution with the context created previously. Stage executions are created by calling the create_execution method. execution = cmf . create_execution ( execution_type = \"download\" , # Execution name. custom_properties = { # Execution parameters \"url\" : \"https://a.com/mnist.gz\" # Data URL. } ) Log artifacts . A stage execution can consume (inputs) and produce (outputs) multiple artifacts (datasets, models and performance metrics). The path of these artifacts must be relative to the project (repository) root path. Artifacts might have optional metadata associated with them. These metadata could include feature statistics for ML datasets, or useful parameters for ML models (such as, for instance, number of trees in a random forest classifier). Datasets are logged with the log_dataset method. cmf . log_dataset ( 'data/mnist.gz' , \"input\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : 'raw' }) cmf . log_dataset ( 'data/train.csv' , \"output\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : \"train_split\" }) cmf . log_dataset ( 'data/test.csv' , \"output\" , custom_properties = { \"name\" : \"mnist\" , \"type\" : \"test_split\" }) ML models produced by training stages are logged using log_model API. ML models can be both input and output artifacts. The metadata associated with the artifact could be logged as an optional argument. # In train stage cmf . log_model ( path = \"model/rf.pkl\" , event = \"output\" , model_framework = \"scikit-learn\" , model_type = \"RandomForestClassifier\" , model_name = \"RandomForestClassifier:default\" ) # In test stage cmf . log_model ( path = \"model/rf.pkl\" , event = \"input\" ) Metrics of every optimization step (one epoch of Stochastic Gradient Descent, or one boosting round in Gradient Boosting Trees) are logged using log_metric API. #Can be called at every epoch or every step in the training. This is logged to a parquet file and committed at the # commit stage. #Inside training loop while True : cmf . log_metric ( \"training_metrics\" , { \"loss\" : loss }) cmf . commit_metrics ( \"training_metrics\" ) Stage metrics , or final metrics, are logged with the log_execution_metrics method. These are final metrics of a stage, such as final train or test accuracy. cmf . log_execution_metrics ( \"metrics\" , { \"avg_prec\" : avg_prec , \"roc_auc\" : roc_auc }) Dataslices are intended to be used to track subsets of the data. For instance, this can be used to track and compare accuracies of ML models on these subsets to identify model bias. Data slices are created with the create_dataslice method. dataslice = cmf . create_dataslice ( \"slice-a\" ) for i in range ( 1 , 20 , 1 ): j = random . randrange ( 100 ) dataslice . add_data ( \"data/raw_data/\" + str ( j ) + \".xml\" ) dataslice . commit ()","title":"API Overview"},{"location":"#graph-layer-overview","text":"CMF library has an optional graph layer which stores the relationships in a Neo4J graph database. To use the graph layer, the graph parameter in the library init call must be set to true (it is set to false by default). The library reads the configuration parameters of the graph database from the following environment variables: NEO4J_URI , NEO4J_USER_NAME and NEO4J_PASSWD . They need to be made available in a user environment, e.g.: export NEO4J_URI = \"bolt://10.93.244.219:7687\" export NEO4J_USER_NAME = neo4j export NEO4J_PASSWD = neo4j To use the graph layer, instantiate the CMF with graph=True parameter: from cmflib import cmf cmf = cmf . Cmf ( filename = \"mlmd\" , pipeline_name = \"anomaly_detection_pipeline\" , graph = True )","title":"Graph Layer Overview"},{"location":"#use-a-jupyterlab-docker-environment-with-cmf-pre-installed","text":"CMF has a docker-compose file which creates two docker containers, - JupyterLab Notebook Environment with CMF pre installed. - Accessible at http://[HOST.IP.AD.DR]:8888 (default token: docker ) - Within the Jupyterlab environment, a startup script switches context to $USER:$GROUP as specified in .env - example-get-started from this repo is bind mounted into /home/jovyan/example-get-started - Neo4j Docker container to store and access lineages.","title":" Use a Jupyterlab Docker environment with CMF pre-installed"},{"location":"#step-1","text":"create .env file in current folder using env-example as a template. Modify the .env file for the following variables USER,UID,GROUP,GID,GIT_USER_NAME,GIT_USER_EMAIL,GIT_REMOTE_URL #These are used by docker-compose.yml","title":"Step 1. "},{"location":"#step-2","text":"Update docker-compose.yml as needed. your .ssh folder is mounted inside the docker conatiner to enable you to push and pull code from git To-Do Create these directories in your home folder mkdir $HOME/workspace mkdir $HOME/dvc_remote workspace - workspace will be mounted inside the cmf pre-installed docker conatiner (can be your code directory) dvc_remote - remote data store for dvc or Change the below lines in docker-compose to reflect the appropriate directories If your workspace is named \"experiment\" change the below line $HOME/workspace:/home/jovyan/workspace to $HOME/experiment:/home/jovyan/wokspace If your remote is /extmount/data change the line $HOME/dvc_remote:/home/jovyan/dvc_remote to /extmount/data:/home/jovyan/dvc_remote Start the docker docker-compose up --build -d Access the jupyter notebook http://[HOST.IP.AD.DR]:8888 (default token: docker ) Click the terminal icon Quick Start cd example-get-started sh initialize.sh sh test_script.sh dvc push The above steps will run a pre coded example pipeline and the metadata is stored in a file named \"mlmd\". The artifacts created will be pushed to configured dvc remote (default: /home/dvc_remote) The stored metadata is displayed as Metadata lineage can be accessed in neo4j. Open http://host:7475/browser/ Connect to server with default password neo4j123 (To change this modify .env file) Run the query MATCH (a:Execution)-[r]-(b) WHERE (b:Dataset or b:Model or b:Metrics) RETURN a,r, b Expected output Jupyter Lab Notebook Select the kernel as Python[conda env:python37] Shutdown/remove (Remove volumes as well) docker-compose down -v","title":"Step 2. "},{"location":"#license","text":"CMF is an open source project hosted on GitHub and distributed according to the Apache 2.0 licence . We are welcome user contributions - send us a message on the Slack channel or open a GitHub issue or a pull request on GitHub.","title":"License"},{"location":"#citation","text":"@mist { foltin2022cmf , title = {Self-Learning Data Foundation for Scientific AI} , author = {Martin Foltin, Annmary Justine, Sergey Serebryakov, Cong Xu, Aalap Tripathy, Suparna Bhattacharya, Paolo Faraboschi} , year = {2022} , note = {Presented at the \"Monterey Data Conference\"} , URL = {https://drive.google.com/file/d/1Oqs0AN0RsAjt_y9ZjzYOmBxI8H0yqSpB/view} , }","title":"Citation"},{"location":"#community","text":"Help Common Metadata Framework and its documentation are in active stage of development and are very new. If there is anything unclear, missing or there's a typo, please, open an issue or pull request on GitHub .","title":"Community"},{"location":"_src/","text":"CMF docs development resources \u00b6 This directory contains files that are used to create some content for the CMF documentation. This process is not automated yet. Files in this directory are not supposed to be referenced from documentation pages. It also should not be required to automatically redeploy documentation (e.g., with GitHub actions) when documentation files change only in this particular directory. The diagrams.drawio file is created with PyCharm 's Diagram.NET plugin. It contains a number of diagrams used in the documentation. Now, to update those diagrams, use this file to edit them, them take a screenshot, edit with some editor, and then overwrite corresponding files (e.g., ML Pipeline Definition ) used on the main page.","title":"CMF docs development resources"},{"location":"_src/#cmf-docs-development-resources","text":"This directory contains files that are used to create some content for the CMF documentation. This process is not automated yet. Files in this directory are not supposed to be referenced from documentation pages. It also should not be required to automatically redeploy documentation (e.g., with GitHub actions) when documentation files change only in this particular directory. The diagrams.drawio file is created with PyCharm 's Diagram.NET plugin. It contains a number of diagrams used in the documentation. Now, to update those diagrams, use this file to edit them, them take a screenshot, edit with some editor, and then overwrite corresponding files (e.g., ML Pipeline Definition ) used on the main page.","title":"CMF docs development resources"},{"location":"api/public/cmf/","text":"cmflib.cmf.Cmf \u00b6 This class provides methods to log metadata for distributed AI pipelines. The class instance creates an ML metadata store to store the metadata. It creates a driver to store nodes and its relationships to neo4j. The user has to provide the name of the pipeline, that needs to be recorded with it. cmflib . cmf . Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" , custom_properties = { \"owner\" : \"user_a\" }, graph = False ) Parameters: Name Type Description Default filename str Path to the sqlite file to store the metadata 'mlmd' pipeline_name str Name to uniquely identify the pipeline. Note that name is the unique identification for a pipeline. If a pipeline already exist with the same name, the existing pipeline object is reused. '' custom_properties t . Optional [ t . Dict ] Additional properties of the pipeline that needs to be stored. None graph bool If set to true, the libray also stores the relationships in the provided graph database. The following environment variables should be set: NEO4J_URI (graph server URI), NEO4J_USER_NAME (user name) and NEO4J_PASSWD (user password), e.g.: export NEO4J_URI = \"bolt://ip:port\" export NEO4J_USER_NAME = neo4j export NEO4J_PASSWD = neo4j False Source code in cmflib/cmf.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , filename : str = \"mlmd\" , pipeline_name : str = \"\" , custom_properties : t . Optional [ t . Dict ] = None , graph : bool = False ): Cmf . __prechecks () if custom_properties is None : custom_properties = {} config = mlpb . ConnectionConfig () config . sqlite . filename_uri = filename self . store = metadata_store . MetadataStore ( config ) self . filename = filename self . child_context = None self . execution = None self . execution_name = \"\" self . execution_command = \"\" self . metrics = {} self . input_artifacts = [] self . execution_label_props = {} self . graph = graph self . branch_name = filename . rsplit ( '/' , 1 )[ - 1 ] git_checkout_new_branch ( self . branch_name ) self . parent_context = get_or_create_parent_context ( store = self . store , pipeline = pipeline_name , custom_properties = custom_properties ) if graph is True : self . driver = graph_wrapper . GraphDriver ( Cmf . __neo4j_uri , Cmf . __neo4j_user , Cmf . __neo4j_password ) self . driver . create_pipeline_node ( pipeline_name , self . parent_context . id , custom_properties ) create_context ( pipeline_stage , custom_properties = None ) \u00b6 Creates a stage in the pipeline. If it already exists, it is reused and not created again. Example # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" ) # Create or reuse context for this stage context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"prepare\" , custom_properties = { \"user-metadata1\" : \"metadata_value\" } ) Parameters: Name Type Description Default pipeline_stage str Name of the pipeline stage. required custom_properties t . Optional [ t . Dict ] Developers can provide key value pairs with additional properties of the stage that need to be stored. None Returns: Type Description mlpb . Context Context object from ML Metadata library associated with the new stage. Source code in cmflib/cmf.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def create_context ( self , pipeline_stage : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Context : \"\"\"Creates a stage in the pipeline. If it already exists, it is reused and not created again. Example: ```python # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\") # Create or reuse context for this stage context: mlmd.proto.Context = cmf.create_context( pipeline_stage=\"prepare\", custom_properties ={\"user-metadata1\": \"metadata_value\"} ) ``` Args: pipeline_stage: Name of the pipeline stage. custom_properties: Developers can provide key value pairs with additional properties of the stage that need to be stored. Returns: Context object from ML Metadata library associated with the new stage. \"\"\" custom_props = {} if custom_properties is None else custom_properties ctx = get_or_create_run_context ( self . store , pipeline_stage , custom_props ) self . child_context = ctx associate_child_to_parent_context ( store = self . store , parent_context = self . parent_context , child_context = ctx ) if self . graph : self . driver . create_stage_node ( pipeline_stage , self . parent_context , ctx . id , custom_props ) return ctx create_execution ( execution_type , custom_properties = None ) \u00b6 Create execution. Every call creates a unique execution. Execution can only be created within a context, so create_context must be called first. Example # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" ) # Create or reuse context for this stage context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"prepare\" , custom_properties = { \"user-metadata1\" : \"metadata_value\" } ) # Create a new execution for this stage run execution : mlmd . proto . Execution = cmf . create_execution ( execution_type = \"Prepare\" , custom_properties = { \"split\" : split , \"seed\" : seed } ) Parameters: Name Type Description Default execution_type str Name of the execution. required custom_properties t . Optional [ t . Dict ] Developers can provide key value pairs with additional properties of the execution that need to be stored. None Returns: Type Description mlpb . Execution Execution object from ML Metadata library associated with the new execution for this stage. Source code in cmflib/cmf.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def create_execution ( self , execution_type : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Execution : \"\"\"Create execution. Every call creates a unique execution. Execution can only be created within a context, so [create_context][cmflib.cmf.Cmf.create_context] must be called first. Example: ```python # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\") # Create or reuse context for this stage context: mlmd.proto.Context = cmf.create_context( pipeline_stage=\"prepare\", custom_properties ={\"user-metadata1\": \"metadata_value\"} ) # Create a new execution for this stage run execution: mlmd.proto.Execution = cmf.create_execution( execution_type=\"Prepare\", custom_properties = {\"split\": split, \"seed\": seed} ) ``` Args: execution_type: Name of the execution. custom_properties: Developers can provide key value pairs with additional properties of the execution that need to be stored. Returns: Execution object from ML Metadata library associated with the new execution for this stage. \"\"\" # Initializing the execution related fields self . metrics = {} self . input_artifacts = [] self . execution_label_props = {} custom_props = {} if custom_properties is None else custom_properties git_repo = git_get_repo () git_start_commit = git_get_commit () self . execution = create_new_execution_in_existing_run_context ( store = self . store , execution_type_name = execution_type , context_id = self . child_context . id , execution = str ( sys . argv ), pipeline_id = self . parent_context . id , pipeline_type = self . parent_context . name , git_repo = git_repo , git_start_commit = git_start_commit , custom_properties = custom_props ) self . execution_name = str ( self . execution . id ) + \",\" + execution_type self . execution_command = str ( sys . argv ) for k , v in custom_props . items (): k = re . sub ( '-' , '_' , k ) self . execution_label_props [ k ] = v self . execution_label_props [ \"Execution_Name\" ] = execution_type + \\ \":\" + str ( self . execution . id ) self . execution_label_props [ \"execution_command\" ] = str ( sys . argv ) if self . graph : self . driver . create_execution_node ( self . execution_name , self . child_context . id , self . parent_context , str ( sys . argv ), self . execution . id , custom_props ) return self . execution log_dataset ( url , event , custom_properties = None ) \u00b6 Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata. Example artifact : mlmd . proto . Artifact = cmf . log_dataset ( url = \"/repo/data.xml\" , event = \"input\" , custom_properties = { \"source\" : \"kaggle\" } ) Parameters: Name Type Description Default url str The path to the dataset. required event str Takes arguments INPUT OR OUTPUT . required custom_properties t . Optional [ t . Dict ] Dataset properties (key/value pairs). None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new dataset artifact. Source code in cmflib/cmf.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def log_dataset ( self , url : str , event : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata. Example: ```python artifact: mlmd.proto.Artifact = cmf.log_dataset( url=\"/repo/data.xml\", event=\"input\", custom_properties={\"source\":\"kaggle\"} ) ``` Args: url: The path to the dataset. event: Takes arguments `INPUT` OR `OUTPUT`. custom_properties: Dataset properties (key/value pairs). Returns: Artifact object from ML Metadata library associated with the new dataset artifact. \"\"\" custom_props = {} if custom_properties is None else custom_properties git_repo = git_get_repo () name = re . split ( '/' , url )[ - 1 ] event_type = mlpb . Event . Type . OUTPUT existing_artifact = [] if event . lower () == \"input\" : event_type = mlpb . Event . Type . INPUT dataset_commit = commit_output ( url , self . execution . id ) c_hash = dvc_get_hash ( url ) url = url + \":\" + c_hash if c_hash and c_hash . strip : existing_artifact . extend ( self . store . get_artifacts_by_uri ( c_hash )) # To Do - What happens when uri is the same but names are different if existing_artifact and len ( existing_artifact ) != 0 : existing_artifact = existing_artifact [ 0 ] # Quick fix- Updating only the name if custom_properties is not None : self . update_existing_artifact ( existing_artifact , custom_properties ) uri = c_hash artifact = link_execution_to_artifact ( store = self . store , execution_id = self . execution . id , uri = uri , input_name = url , event_type = event_type ) else : # if((existing_artifact and len(existing_artifact )!= 0) and c_hash != \"\"): # url = url + \":\" + str(self.execution.id) uri = c_hash if c_hash and c_hash . strip () else str ( uuid . uuid1 ()) artifact = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = url , type_name = \"Dataset\" , event_type = event_type , properties = { \"git_repo\" : str ( git_repo ), \"Commit\" : str ( dataset_commit )}, artifact_type_properties = { \"git_repo\" : mlpb . STRING , \"Commit\" : mlpb . STRING }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) custom_props [ \"git_repo\" ] = git_repo custom_props [ \"Commit\" ] = dataset_commit self . execution_label_props [ \"git_repo\" ] = git_repo self . execution_label_props [ \"Commit\" ] = dataset_commit if self . graph : self . driver . create_dataset_node ( name , url , uri , event , self . execution . id , self . parent_context , custom_props ) if event . lower () == \"input\" : self . input_artifacts . append ({ \"Name\" : name , \"Path\" : url , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Dataset\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name }) self . driver . create_execution_links ( uri , name , \"Dataset\" ) else : child_artifact = { \"Name\" : name , \"Path\" : url , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Dataset\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return artifact log_model ( path , event , model_framework = 'Default' , model_type = 'Default' , model_name = 'Default' , custom_properties = None ) \u00b6 Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git. Example artifact : mlmd . proto . Artifact = cmf . log_model ( path = \"path/to/model.pkl\" , event = \"output\" , model_framework = \"SKlearn\" , model_type = \"RandomForestClassifier\" , model_name = \"RandomForestClassifier:default\" ) Parameters: Name Type Description Default path str Path to the model file. required event str Takes arguments INPUT OR OUTPUT . required model_framework str Framework used to create the model. 'Default' model_type str Type of model algorithm used. 'Default' model_name str Name of the algorithm used. 'Default' custom_properties t . Optional [ t . Dict ] The model properties. None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new model artifact. Source code in cmflib/cmf.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def log_model ( self , path : str , event : str , model_framework : str = \"Default\" , model_type : str = \"Default\" , model_name : str = \"Default\" , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git. Example: ```python artifact: mlmd.proto.Artifact= cmf.log_model( path=\"path/to/model.pkl\", event=\"output\", model_framework=\"SKlearn\", model_type=\"RandomForestClassifier\", model_name=\"RandomForestClassifier:default\" ) ``` Args: path: Path to the model file. event: Takes arguments `INPUT` OR `OUTPUT`. model_framework: Framework used to create the model. model_type: Type of model algorithm used. model_name: Name of the algorithm used. custom_properties: The model properties. Returns: Artifact object from ML Metadata library associated with the new model artifact. \"\"\" if custom_properties is None : custom_properties = {} custom_props = {} if custom_properties is None else custom_properties # name = re.split('/', path)[-1] event_type = mlpb . Event . Type . OUTPUT existing_artifact = [] if event . lower () == \"input\" : event_type = mlpb . Event . Type . INPUT model_commit = commit_output ( path , self . execution . id ) c_hash = dvc_get_hash ( path ) # If connecting to an existing artifact - The name of the artifact is # used as path/steps/key model_uri = path + \":\" + c_hash # uri = \"\" if c_hash and c_hash . strip (): uri = c_hash . strip () existing_artifact . extend ( self . store . get_artifacts_by_uri ( uri )) else : raise RuntimeError ( \"Model commit failed, Model uri empty\" ) if existing_artifact and len ( existing_artifact ) != 0 and event_type == mlpb . Event . Type . INPUT : artifact = link_execution_to_artifact ( store = self . store , execution_id = self . execution . id , uri = c_hash , input_name = model_uri , event_type = event_type ) model_uri = artifact . name else : uri = c_hash if c_hash and c_hash . strip () else str ( uuid . uuid1 ()) model_uri = model_uri + \":\" + str ( self . execution . id ) artifact = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = model_uri , type_name = \"Model\" , event_type = event_type , properties = { \"model_framework\" : str ( model_framework ), \"model_type\" : str ( model_type ), \"model_name\" : str ( model_name ), \"Commit\" : str ( model_commit )}, artifact_type_properties = { \"model_framework\" : mlpb . STRING , \"model_type\" : mlpb . STRING , \"model_name\" : mlpb . STRING , \"Commit\" : mlpb . STRING , }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) # custom_properties[\"Commit\"] = model_commit self . execution_label_props [ \"Commit\" ] = model_commit if self . graph : self . driver . create_model_node ( model_uri , uri , event , self . execution . id , self . parent_context , custom_props ) if event . lower () == \"input\" : self . input_artifacts . append ( { \"Name\" : model_uri , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Model\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name }) self . driver . create_execution_links ( uri , model_uri , \"Model\" ) else : child_artifact = { \"Name\" : model_uri , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Model\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return artifact log_execution_metrics ( metrics_name , custom_properties = None ) \u00b6 Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have. Example exec_metrics : mlpb . Artifact = cmf . log_execution_metrics ( metrics_name = \"Training_Metrics\" , { \"auc\" : auc , \"loss\" : loss } ) Parameters: Name Type Description Default metrics_name str Name to identify the metrics. required custom_properties t . Optional [ t . Dict ] Dictionary with metric values. None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact. Source code in cmflib/cmf.py 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def log_execution_metrics ( self , metrics_name : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have. Example: ```python exec_metrics: mlpb.Artifact = cmf.log_execution_metrics( metrics_name=\"Training_Metrics\", {\"auc\": auc, \"loss\": loss} ) ``` Args: metrics_name: Name to identify the metrics. custom_properties: Dictionary with metric values. Returns: Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact. \"\"\" custom_props = {} if custom_properties is None else custom_properties uri = str ( uuid . uuid1 ()) metrics_name = metrics_name + \":\" + uri + \":\" + str ( self . execution . id ) metrics = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = metrics_name , type_name = \"Metrics\" , event_type = mlpb . Event . Type . OUTPUT , properties = { \"metrics_name\" : metrics_name }, artifact_type_properties = { \"metrics_name\" : mlpb . STRING }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) if self . graph : # To do create execution_links self . driver . create_metrics_node ( metrics_name , uri , \"output\" , self . execution . id , self . parent_context , custom_props ) child_artifact = { \"Name\" : metrics_name , \"URI\" : uri , \"Event\" : \"output\" , \"Execution_Name\" : self . execution_name , \"Type\" : \"Metrics\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return metrics log_metric ( metrics_name , custom_properties = None ) \u00b6 Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The commit_metrics call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the read_metrics call. Example # Can be called at every epoch or every step in the training. This is logged to a parquet file and committed # at the commit stage. # Inside training loop while True : cmf . log_metric ( \"training_metrics\" , { \"train_loss\" : train_loss }) cmf . commit_metrics ( \"training_metrics\" ) Parameters: Name Type Description Default metrics_name str Name to identify the metrics. required custom_properties t . Optional [ t . Dict ] Dictionary with metrics. None Source code in cmflib/cmf.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 def log_metric ( self , metrics_name : str , custom_properties : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The `commit_metrics` call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the `read_metrics` call. Example: ```python # Can be called at every epoch or every step in the training. This is logged to a parquet file and committed # at the commit stage. # Inside training loop while True: cmf.log_metric(\"training_metrics\", {\"train_loss\": train_loss}) cmf.commit_metrics(\"training_metrics\") ``` Args: metrics_name: Name to identify the metrics. custom_properties: Dictionary with metrics. \"\"\" if metrics_name in self . metrics : key = max (( self . metrics [ metrics_name ]) . keys ()) + 1 self . metrics [ metrics_name ][ key ] = custom_properties else : self . metrics [ metrics_name ] = {} self . metrics [ metrics_name ][ 1 ] = custom_properties create_dataslice ( name ) \u00b6 Creates a dataslice object. Once created, users can add data instances to this data slice with add_data method. Users are also responsible for committing data slices by calling the commit method. Example dataslice = cmf . create_dataslice ( \"slice-a\" ) Parameters: Name Type Description Default name str Name to identify the dataslice. required Returns: Type Description DataSlice Instance of a newly created DataSlice . Source code in cmflib/cmf.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 def create_dataslice ( self , name : str ) -> \"Cmf.DataSlice\" : \"\"\"Creates a dataslice object. Once created, users can add data instances to this data slice with [add_data][cmflib.cmf.Cmf.DataSlice.add_data] method. Users are also responsible for committing data slices by calling the [commit][cmflib.cmf.Cmf.DataSlice.commit] method. Example: ```python dataslice = cmf.create_dataslice(\"slice-a\") ``` Args: name: Name to identify the dataslice. Returns: Instance of a newly created [DataSlice][cmflib.cmf.Cmf.DataSlice]. \"\"\" return Cmf . DataSlice ( name , self )","title":"CMF"},{"location":"api/public/cmf/#cmflibcmfcmf","text":"This class provides methods to log metadata for distributed AI pipelines. The class instance creates an ML metadata store to store the metadata. It creates a driver to store nodes and its relationships to neo4j. The user has to provide the name of the pipeline, that needs to be recorded with it. cmflib . cmf . Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" , custom_properties = { \"owner\" : \"user_a\" }, graph = False ) Parameters: Name Type Description Default filename str Path to the sqlite file to store the metadata 'mlmd' pipeline_name str Name to uniquely identify the pipeline. Note that name is the unique identification for a pipeline. If a pipeline already exist with the same name, the existing pipeline object is reused. '' custom_properties t . Optional [ t . Dict ] Additional properties of the pipeline that needs to be stored. None graph bool If set to true, the libray also stores the relationships in the provided graph database. The following environment variables should be set: NEO4J_URI (graph server URI), NEO4J_USER_NAME (user name) and NEO4J_PASSWD (user password), e.g.: export NEO4J_URI = \"bolt://ip:port\" export NEO4J_USER_NAME = neo4j export NEO4J_PASSWD = neo4j False Source code in cmflib/cmf.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , filename : str = \"mlmd\" , pipeline_name : str = \"\" , custom_properties : t . Optional [ t . Dict ] = None , graph : bool = False ): Cmf . __prechecks () if custom_properties is None : custom_properties = {} config = mlpb . ConnectionConfig () config . sqlite . filename_uri = filename self . store = metadata_store . MetadataStore ( config ) self . filename = filename self . child_context = None self . execution = None self . execution_name = \"\" self . execution_command = \"\" self . metrics = {} self . input_artifacts = [] self . execution_label_props = {} self . graph = graph self . branch_name = filename . rsplit ( '/' , 1 )[ - 1 ] git_checkout_new_branch ( self . branch_name ) self . parent_context = get_or_create_parent_context ( store = self . store , pipeline = pipeline_name , custom_properties = custom_properties ) if graph is True : self . driver = graph_wrapper . GraphDriver ( Cmf . __neo4j_uri , Cmf . __neo4j_user , Cmf . __neo4j_password ) self . driver . create_pipeline_node ( pipeline_name , self . parent_context . id , custom_properties )","title":"cmflib.cmf.Cmf"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_context","text":"Creates a stage in the pipeline. If it already exists, it is reused and not created again. Example # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" ) # Create or reuse context for this stage context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"prepare\" , custom_properties = { \"user-metadata1\" : \"metadata_value\" } ) Parameters: Name Type Description Default pipeline_stage str Name of the pipeline stage. required custom_properties t . Optional [ t . Dict ] Developers can provide key value pairs with additional properties of the stage that need to be stored. None Returns: Type Description mlpb . Context Context object from ML Metadata library associated with the new stage. Source code in cmflib/cmf.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def create_context ( self , pipeline_stage : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Context : \"\"\"Creates a stage in the pipeline. If it already exists, it is reused and not created again. Example: ```python # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\") # Create or reuse context for this stage context: mlmd.proto.Context = cmf.create_context( pipeline_stage=\"prepare\", custom_properties ={\"user-metadata1\": \"metadata_value\"} ) ``` Args: pipeline_stage: Name of the pipeline stage. custom_properties: Developers can provide key value pairs with additional properties of the stage that need to be stored. Returns: Context object from ML Metadata library associated with the new stage. \"\"\" custom_props = {} if custom_properties is None else custom_properties ctx = get_or_create_run_context ( self . store , pipeline_stage , custom_props ) self . child_context = ctx associate_child_to_parent_context ( store = self . store , parent_context = self . parent_context , child_context = ctx ) if self . graph : self . driver . create_stage_node ( pipeline_stage , self . parent_context , ctx . id , custom_props ) return ctx","title":"create_context()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_execution","text":"Create execution. Every call creates a unique execution. Execution can only be created within a context, so create_context must be called first. Example # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf ( filename = \"mlmd\" , pipeline_name = \"test_pipeline\" ) # Create or reuse context for this stage context : mlmd . proto . Context = cmf . create_context ( pipeline_stage = \"prepare\" , custom_properties = { \"user-metadata1\" : \"metadata_value\" } ) # Create a new execution for this stage run execution : mlmd . proto . Execution = cmf . create_execution ( execution_type = \"Prepare\" , custom_properties = { \"split\" : split , \"seed\" : seed } ) Parameters: Name Type Description Default execution_type str Name of the execution. required custom_properties t . Optional [ t . Dict ] Developers can provide key value pairs with additional properties of the execution that need to be stored. None Returns: Type Description mlpb . Execution Execution object from ML Metadata library associated with the new execution for this stage. Source code in cmflib/cmf.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def create_execution ( self , execution_type : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Execution : \"\"\"Create execution. Every call creates a unique execution. Execution can only be created within a context, so [create_context][cmflib.cmf.Cmf.create_context] must be called first. Example: ```python # Import CMF from cmflib.cmf import Cmf from ml_metadata.proto import metadata_store_pb2 as mlpb # Create CMF logger cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\") # Create or reuse context for this stage context: mlmd.proto.Context = cmf.create_context( pipeline_stage=\"prepare\", custom_properties ={\"user-metadata1\": \"metadata_value\"} ) # Create a new execution for this stage run execution: mlmd.proto.Execution = cmf.create_execution( execution_type=\"Prepare\", custom_properties = {\"split\": split, \"seed\": seed} ) ``` Args: execution_type: Name of the execution. custom_properties: Developers can provide key value pairs with additional properties of the execution that need to be stored. Returns: Execution object from ML Metadata library associated with the new execution for this stage. \"\"\" # Initializing the execution related fields self . metrics = {} self . input_artifacts = [] self . execution_label_props = {} custom_props = {} if custom_properties is None else custom_properties git_repo = git_get_repo () git_start_commit = git_get_commit () self . execution = create_new_execution_in_existing_run_context ( store = self . store , execution_type_name = execution_type , context_id = self . child_context . id , execution = str ( sys . argv ), pipeline_id = self . parent_context . id , pipeline_type = self . parent_context . name , git_repo = git_repo , git_start_commit = git_start_commit , custom_properties = custom_props ) self . execution_name = str ( self . execution . id ) + \",\" + execution_type self . execution_command = str ( sys . argv ) for k , v in custom_props . items (): k = re . sub ( '-' , '_' , k ) self . execution_label_props [ k ] = v self . execution_label_props [ \"Execution_Name\" ] = execution_type + \\ \":\" + str ( self . execution . id ) self . execution_label_props [ \"execution_command\" ] = str ( sys . argv ) if self . graph : self . driver . create_execution_node ( self . execution_name , self . child_context . id , self . parent_context , str ( sys . argv ), self . execution . id , custom_props ) return self . execution","title":"create_execution()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_dataset","text":"Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata. Example artifact : mlmd . proto . Artifact = cmf . log_dataset ( url = \"/repo/data.xml\" , event = \"input\" , custom_properties = { \"source\" : \"kaggle\" } ) Parameters: Name Type Description Default url str The path to the dataset. required event str Takes arguments INPUT OR OUTPUT . required custom_properties t . Optional [ t . Dict ] Dataset properties (key/value pairs). None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new dataset artifact. Source code in cmflib/cmf.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def log_dataset ( self , url : str , event : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata. Example: ```python artifact: mlmd.proto.Artifact = cmf.log_dataset( url=\"/repo/data.xml\", event=\"input\", custom_properties={\"source\":\"kaggle\"} ) ``` Args: url: The path to the dataset. event: Takes arguments `INPUT` OR `OUTPUT`. custom_properties: Dataset properties (key/value pairs). Returns: Artifact object from ML Metadata library associated with the new dataset artifact. \"\"\" custom_props = {} if custom_properties is None else custom_properties git_repo = git_get_repo () name = re . split ( '/' , url )[ - 1 ] event_type = mlpb . Event . Type . OUTPUT existing_artifact = [] if event . lower () == \"input\" : event_type = mlpb . Event . Type . INPUT dataset_commit = commit_output ( url , self . execution . id ) c_hash = dvc_get_hash ( url ) url = url + \":\" + c_hash if c_hash and c_hash . strip : existing_artifact . extend ( self . store . get_artifacts_by_uri ( c_hash )) # To Do - What happens when uri is the same but names are different if existing_artifact and len ( existing_artifact ) != 0 : existing_artifact = existing_artifact [ 0 ] # Quick fix- Updating only the name if custom_properties is not None : self . update_existing_artifact ( existing_artifact , custom_properties ) uri = c_hash artifact = link_execution_to_artifact ( store = self . store , execution_id = self . execution . id , uri = uri , input_name = url , event_type = event_type ) else : # if((existing_artifact and len(existing_artifact )!= 0) and c_hash != \"\"): # url = url + \":\" + str(self.execution.id) uri = c_hash if c_hash and c_hash . strip () else str ( uuid . uuid1 ()) artifact = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = url , type_name = \"Dataset\" , event_type = event_type , properties = { \"git_repo\" : str ( git_repo ), \"Commit\" : str ( dataset_commit )}, artifact_type_properties = { \"git_repo\" : mlpb . STRING , \"Commit\" : mlpb . STRING }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) custom_props [ \"git_repo\" ] = git_repo custom_props [ \"Commit\" ] = dataset_commit self . execution_label_props [ \"git_repo\" ] = git_repo self . execution_label_props [ \"Commit\" ] = dataset_commit if self . graph : self . driver . create_dataset_node ( name , url , uri , event , self . execution . id , self . parent_context , custom_props ) if event . lower () == \"input\" : self . input_artifacts . append ({ \"Name\" : name , \"Path\" : url , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Dataset\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name }) self . driver . create_execution_links ( uri , name , \"Dataset\" ) else : child_artifact = { \"Name\" : name , \"Path\" : url , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Dataset\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return artifact","title":"log_dataset()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_model","text":"Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git. Example artifact : mlmd . proto . Artifact = cmf . log_model ( path = \"path/to/model.pkl\" , event = \"output\" , model_framework = \"SKlearn\" , model_type = \"RandomForestClassifier\" , model_name = \"RandomForestClassifier:default\" ) Parameters: Name Type Description Default path str Path to the model file. required event str Takes arguments INPUT OR OUTPUT . required model_framework str Framework used to create the model. 'Default' model_type str Type of model algorithm used. 'Default' model_name str Name of the algorithm used. 'Default' custom_properties t . Optional [ t . Dict ] The model properties. None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new model artifact. Source code in cmflib/cmf.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def log_model ( self , path : str , event : str , model_framework : str = \"Default\" , model_type : str = \"Default\" , model_name : str = \"Default\" , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git. Example: ```python artifact: mlmd.proto.Artifact= cmf.log_model( path=\"path/to/model.pkl\", event=\"output\", model_framework=\"SKlearn\", model_type=\"RandomForestClassifier\", model_name=\"RandomForestClassifier:default\" ) ``` Args: path: Path to the model file. event: Takes arguments `INPUT` OR `OUTPUT`. model_framework: Framework used to create the model. model_type: Type of model algorithm used. model_name: Name of the algorithm used. custom_properties: The model properties. Returns: Artifact object from ML Metadata library associated with the new model artifact. \"\"\" if custom_properties is None : custom_properties = {} custom_props = {} if custom_properties is None else custom_properties # name = re.split('/', path)[-1] event_type = mlpb . Event . Type . OUTPUT existing_artifact = [] if event . lower () == \"input\" : event_type = mlpb . Event . Type . INPUT model_commit = commit_output ( path , self . execution . id ) c_hash = dvc_get_hash ( path ) # If connecting to an existing artifact - The name of the artifact is # used as path/steps/key model_uri = path + \":\" + c_hash # uri = \"\" if c_hash and c_hash . strip (): uri = c_hash . strip () existing_artifact . extend ( self . store . get_artifacts_by_uri ( uri )) else : raise RuntimeError ( \"Model commit failed, Model uri empty\" ) if existing_artifact and len ( existing_artifact ) != 0 and event_type == mlpb . Event . Type . INPUT : artifact = link_execution_to_artifact ( store = self . store , execution_id = self . execution . id , uri = c_hash , input_name = model_uri , event_type = event_type ) model_uri = artifact . name else : uri = c_hash if c_hash and c_hash . strip () else str ( uuid . uuid1 ()) model_uri = model_uri + \":\" + str ( self . execution . id ) artifact = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = model_uri , type_name = \"Model\" , event_type = event_type , properties = { \"model_framework\" : str ( model_framework ), \"model_type\" : str ( model_type ), \"model_name\" : str ( model_name ), \"Commit\" : str ( model_commit )}, artifact_type_properties = { \"model_framework\" : mlpb . STRING , \"model_type\" : mlpb . STRING , \"model_name\" : mlpb . STRING , \"Commit\" : mlpb . STRING , }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) # custom_properties[\"Commit\"] = model_commit self . execution_label_props [ \"Commit\" ] = model_commit if self . graph : self . driver . create_model_node ( model_uri , uri , event , self . execution . id , self . parent_context , custom_props ) if event . lower () == \"input\" : self . input_artifacts . append ( { \"Name\" : model_uri , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Model\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name }) self . driver . create_execution_links ( uri , model_uri , \"Model\" ) else : child_artifact = { \"Name\" : model_uri , \"URI\" : uri , \"Event\" : event . lower (), \"Execution_Name\" : self . execution_name , \"Type\" : \"Model\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return artifact","title":"log_model()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_execution_metrics","text":"Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have. Example exec_metrics : mlpb . Artifact = cmf . log_execution_metrics ( metrics_name = \"Training_Metrics\" , { \"auc\" : auc , \"loss\" : loss } ) Parameters: Name Type Description Default metrics_name str Name to identify the metrics. required custom_properties t . Optional [ t . Dict ] Dictionary with metric values. None Returns: Type Description mlpb . Artifact Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact. Source code in cmflib/cmf.py 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def log_execution_metrics ( self , metrics_name : str , custom_properties : t . Optional [ t . Dict ] = None ) -> mlpb . Artifact : \"\"\"Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have. Example: ```python exec_metrics: mlpb.Artifact = cmf.log_execution_metrics( metrics_name=\"Training_Metrics\", {\"auc\": auc, \"loss\": loss} ) ``` Args: metrics_name: Name to identify the metrics. custom_properties: Dictionary with metric values. Returns: Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact. \"\"\" custom_props = {} if custom_properties is None else custom_properties uri = str ( uuid . uuid1 ()) metrics_name = metrics_name + \":\" + uri + \":\" + str ( self . execution . id ) metrics = create_new_artifact_event_and_attribution ( store = self . store , execution_id = self . execution . id , context_id = self . child_context . id , uri = uri , name = metrics_name , type_name = \"Metrics\" , event_type = mlpb . Event . Type . OUTPUT , properties = { \"metrics_name\" : metrics_name }, artifact_type_properties = { \"metrics_name\" : mlpb . STRING }, custom_properties = custom_props , milliseconds_since_epoch = int ( time . time () * 1000 ), ) if self . graph : # To do create execution_links self . driver . create_metrics_node ( metrics_name , uri , \"output\" , self . execution . id , self . parent_context , custom_props ) child_artifact = { \"Name\" : metrics_name , \"URI\" : uri , \"Event\" : \"output\" , \"Execution_Name\" : self . execution_name , \"Type\" : \"Metrics\" , \"Execution_Command\" : self . execution_command , \"Pipeline_Id\" : self . parent_context . id , \"Pipeline_Name\" : self . parent_context . name } self . driver . create_artifact_relationships ( self . input_artifacts , child_artifact , self . execution_label_props ) return metrics","title":"log_execution_metrics()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_metric","text":"Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The commit_metrics call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the read_metrics call. Example # Can be called at every epoch or every step in the training. This is logged to a parquet file and committed # at the commit stage. # Inside training loop while True : cmf . log_metric ( \"training_metrics\" , { \"train_loss\" : train_loss }) cmf . commit_metrics ( \"training_metrics\" ) Parameters: Name Type Description Default metrics_name str Name to identify the metrics. required custom_properties t . Optional [ t . Dict ] Dictionary with metrics. None Source code in cmflib/cmf.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 def log_metric ( self , metrics_name : str , custom_properties : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The `commit_metrics` call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the `read_metrics` call. Example: ```python # Can be called at every epoch or every step in the training. This is logged to a parquet file and committed # at the commit stage. # Inside training loop while True: cmf.log_metric(\"training_metrics\", {\"train_loss\": train_loss}) cmf.commit_metrics(\"training_metrics\") ``` Args: metrics_name: Name to identify the metrics. custom_properties: Dictionary with metrics. \"\"\" if metrics_name in self . metrics : key = max (( self . metrics [ metrics_name ]) . keys ()) + 1 self . metrics [ metrics_name ][ key ] = custom_properties else : self . metrics [ metrics_name ] = {} self . metrics [ metrics_name ][ 1 ] = custom_properties","title":"log_metric()"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_dataslice","text":"Creates a dataslice object. Once created, users can add data instances to this data slice with add_data method. Users are also responsible for committing data slices by calling the commit method. Example dataslice = cmf . create_dataslice ( \"slice-a\" ) Parameters: Name Type Description Default name str Name to identify the dataslice. required Returns: Type Description DataSlice Instance of a newly created DataSlice . Source code in cmflib/cmf.py 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 def create_dataslice ( self , name : str ) -> \"Cmf.DataSlice\" : \"\"\"Creates a dataslice object. Once created, users can add data instances to this data slice with [add_data][cmflib.cmf.Cmf.DataSlice.add_data] method. Users are also responsible for committing data slices by calling the [commit][cmflib.cmf.Cmf.DataSlice.commit] method. Example: ```python dataslice = cmf.create_dataslice(\"slice-a\") ``` Args: name: Name to identify the dataslice. Returns: Instance of a newly created [DataSlice][cmflib.cmf.Cmf.DataSlice]. \"\"\" return Cmf . DataSlice ( name , self )","title":"create_dataslice()"},{"location":"api/public/dataslice/","text":"cmflib.cmf.Cmf.DataSlice \u00b6 A data slice represents a named subset of data. It can be used to track performance of an ML model on different slices of the training or testing dataset splits. This can be useful from different perspectives, for instance, to mitigate model bias. Instances of data slices are not meant to be created manually by users. Instead, use Cmf.create_dataslice method. Source code in cmflib/cmf.py 886 887 888 889 def __init__ ( self , name : str , writer , props : t . Optional [ t . Dict ] = None ): self . props = {} if props is None else props self . name = name self . writer = writer add_data ( path , custom_props = None ) \u00b6 Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned. Example dataslice . add_data ( f \"data/raw_data/ { j } .xml) Parameters: Name Type Description Default path str Name to identify the file to be added to the dataslice. required custom_props t . Optional [ t . Dict ] Properties associated with this datum. None Source code in cmflib/cmf.py 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def add_data ( self , path : str , custom_props : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned. Example: ```python dataslice.add_data(f\"data/raw_data/{j}.xml) ``` Args: path: Name to identify the file to be added to the dataslice. custom_props: Properties associated with this datum. \"\"\" self . props [ path ] = {} self . props [ path ][ 'hash' ] = dvc_get_hash ( path ) if custom_props : for k , v in custom_props . items (): self . props [ path ][ k ] = v commit ( custom_props = None ) \u00b6 Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software. Example dataslice . commit () Parameters: Name Type Description Default custom_props t . Optional [ t . Dict ] Properties associated with this data slice. None Source code in cmflib/cmf.py 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 def commit ( self , custom_props : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software. Example: ```python dataslice.commit() ``` Args: custom_props: Properties associated with this data slice. \"\"\" git_repo = git_get_repo () dataslice_df = pd . DataFrame . from_dict ( self . props , orient = 'index' ) dataslice_df . index . names = [ 'Path' ] dataslice_df . to_parquet ( self . name ) existing_artifact = [] dataslice_commit = commit_output ( self . name , self . writer . execution . id ) c_hash = dvc_get_hash ( self . name ) remote = dvc_get_url ( self . name ) if c_hash and c_hash . strip (): existing_artifact . extend ( self . writer . store . get_artifacts_by_uri ( c_hash )) if existing_artifact and len ( existing_artifact ) != 0 : print ( \"Adding to existing data slice\" ) _ = link_execution_to_input_artifact ( store = self . writer . store , execution_id = self . writer . execution . id , uri = c_hash , input_name = self . name + \":\" + c_hash ) else : props = { \"Commit\" : dataslice_commit , \"git_repo\" : git_repo , \"Remote\" : remote } custom_properties = props . update ( custom_props ) if custom_props else props create_new_artifact_event_and_attribution ( store = self . writer . store , execution_id = self . writer . execution . id , context_id = self . writer . child_context . id , uri = c_hash , name = self . name + \":\" + c_hash , type_name = \"Dataslice\" , event_type = mlpb . Event . Type . OUTPUT , custom_properties = custom_properties , milliseconds_since_epoch = int ( time . time () * 1000 ), )","title":"DataSlice"},{"location":"api/public/dataslice/#cmflibcmfcmfdataslice","text":"A data slice represents a named subset of data. It can be used to track performance of an ML model on different slices of the training or testing dataset splits. This can be useful from different perspectives, for instance, to mitigate model bias. Instances of data slices are not meant to be created manually by users. Instead, use Cmf.create_dataslice method. Source code in cmflib/cmf.py 886 887 888 889 def __init__ ( self , name : str , writer , props : t . Optional [ t . Dict ] = None ): self . props = {} if props is None else props self . name = name self . writer = writer","title":"cmflib.cmf.Cmf.DataSlice"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.add_data","text":"Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned. Example dataslice . add_data ( f \"data/raw_data/ { j } .xml) Parameters: Name Type Description Default path str Name to identify the file to be added to the dataslice. required custom_props t . Optional [ t . Dict ] Properties associated with this datum. None Source code in cmflib/cmf.py 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def add_data ( self , path : str , custom_props : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file should already be versioned. Example: ```python dataslice.add_data(f\"data/raw_data/{j}.xml) ``` Args: path: Name to identify the file to be added to the dataslice. custom_props: Properties associated with this datum. \"\"\" self . props [ path ] = {} self . props [ path ][ 'hash' ] = dvc_get_hash ( path ) if custom_props : for k , v in custom_props . items (): self . props [ path ][ k ] = v","title":"add_data()"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.commit","text":"Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software. Example dataslice . commit () Parameters: Name Type Description Default custom_props t . Optional [ t . Dict ] Properties associated with this data slice. None Source code in cmflib/cmf.py 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 def commit ( self , custom_props : t . Optional [ t . Dict ] = None ) -> None : \"\"\"Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software. Example: ```python dataslice.commit() ``` Args: custom_props: Properties associated with this data slice. \"\"\" git_repo = git_get_repo () dataslice_df = pd . DataFrame . from_dict ( self . props , orient = 'index' ) dataslice_df . index . names = [ 'Path' ] dataslice_df . to_parquet ( self . name ) existing_artifact = [] dataslice_commit = commit_output ( self . name , self . writer . execution . id ) c_hash = dvc_get_hash ( self . name ) remote = dvc_get_url ( self . name ) if c_hash and c_hash . strip (): existing_artifact . extend ( self . writer . store . get_artifacts_by_uri ( c_hash )) if existing_artifact and len ( existing_artifact ) != 0 : print ( \"Adding to existing data slice\" ) _ = link_execution_to_input_artifact ( store = self . writer . store , execution_id = self . writer . execution . id , uri = c_hash , input_name = self . name + \":\" + c_hash ) else : props = { \"Commit\" : dataslice_commit , \"git_repo\" : git_repo , \"Remote\" : remote } custom_properties = props . update ( custom_props ) if custom_props else props create_new_artifact_event_and_attribution ( store = self . writer . store , execution_id = self . writer . execution . id , context_id = self . writer . child_context . id , uri = c_hash , name = self . name + \":\" + c_hash , type_name = \"Dataslice\" , event_type = mlpb . Event . Type . OUTPUT , custom_properties = custom_properties , milliseconds_since_epoch = int ( time . time () * 1000 ), )","title":"commit()"},{"location":"architecture/advantages/","text":"Advantages \u00b6 Tracking of metadata for distributed pipeline, thereby enabling efficient pipeline. Enables tracking of code, data and metadata in a single framework. Provides a git like ease of management for metadata. Provides collaboration across teams.","title":"Advantages"},{"location":"architecture/advantages/#advantages","text":"Tracking of metadata for distributed pipeline, thereby enabling efficient pipeline. Enables tracking of code, data and metadata in a single framework. Provides a git like ease of management for metadata. Provides collaboration across teams.","title":"Advantages"},{"location":"architecture/components/","text":"CMF Components \u00b6 Common metadata framework has the following components: Metadata Library exposes API\u2019s to track the pipeline metadata. It also provides API\u2019s to query the stored metadata. Local Client interacts with the server to pull or push metadata from or to the remote store. Central Server interacts with all the remote clients and is responsible to merge the metadata transferred by the remote client and manage the consolidated metadata. Central Repositories hosts the code, data and metadata. Metadata Library \u00b6 The API\u2019s and the abstractions provided by the library enables tracking of pipeline metadata. It tracks the stages in the pipeline, the input and output artifacts at each stage and metrics. The framework allows metrics to be tracked both at coarse and fine-grained intervals. It could be a stage metrics, which could be captured at the end of a stage or fine-grained metrics which is tracked per step (epoch) or at regular intervals during the execution of the stage. The metadata logged through the APIs are written to a backend relational database. The library also provides API\u2019s to query the metadata stored in the relational database for the users to inspect pipelines. In addition to explicit tracking through the API\u2019s library also provides, implicit tracking. The implicit tracking automatically tracks the software version used in the pipelines. The function arguments and function return values can be automatically tracked by adding metadata tracker class decorators on the functions. Before writing the metadata to relational database, the metadata operations are journaled in the metadata journal log. This enables the framework to transfer the local metadata to the central server. All artifacts are versioned with a data versioning framework (for e.g., DVC). The content hash of the artifacts are generated and stored along with the user provided metadata. A special artifact metadata file called a \u201c.dvc\u201d file is created for every artifact (file / folder) which is added to data version management system. The .dvc file contains the content hash of the artifact. For every new execution, the metadata tracker creates a new branch to track the code. The special metadata file created for artifacts, the \u201c.dvc\u201d file is also committed to GIT and its commit id is tracked as a metadata information. The artifacts are versioned through the versioning of its metadata file. Whenever there is a change in the artifact, the metadata file is modified to reflect its current content hash, and the file is tracked as a new version of the metadata file. The metadata tracker automatically tracks the start commit when the library was initialized and creates separate commit for each change in the artifact along the experiment. This helps to track the transformations on the artifacts along the different stages in the pipeline. Local Client \u00b6 The metadata client interacts with the metadata server. It communicates with the server, for synchronization of metadata. After the experiment is completed, the user invokes the \u201cCmf push\u201d command to push the collected metadata to the remote. This transfers the existing metadata journal to the server. The metadata from the central repository can be pulled to the local repository, either using the artifacts or using the project as the identifier or both. When artifact is used as the identifier, all metadata associated with the artifacts currently present in the branch of the cloned Git repository is pulled from the central repository to the local repository. The pulled metadata consist of not only the immediate metadata associated with the artifacts, it contains the metadata of all the artifacts in its chain of lineage. When project is used as the identifier, all the metadata associated with the current branch of the pipeline code that is checked out is pulled to the local repository. Central Server \u00b6 The central server, exposes REST API\u2019s that can be called from the remote clients. This can help in situations where the connectivity between the core datacenter and the remote client is robust. The remote client calls the API\u2019s exposed by the central server to log the metadata directly to the central metadata repository. Where the connectivity with the central server is intermittent, the remote clients log the metadata to the local repository. The journaled metadata is pushed by the remote client to the central server. The central server, will replay the journal and merge the incoming metadata with the metadata already existing in the central repository. The ability to accurately identify the artifacts anywhere using their content hash, makes this merge robust. Central Repositories \u00b6 The common metadata framework consist of three central repositories for the code, data and metadata. Central Metadata repository \u00b6 Central metadata repository holds the metadata pushed from the distributed sites. It holds metadata about all the different pipelines that was tracked using the common metadata tracker. The consolidated view of the metadata stored in the central repository, helps the users to learn across various stages in the pipeline executed at different locations. Using the query layer that is pointed to the central repository, the users gets the global view of the metadata which provides them with a deeper understanding of the pipelines and its metadata. The metadata helps to understand nonobvious results like performance of a dataset with respect to other datasets, Performance of a particular pipeline with respect to other pipelines etc. Central Artifact storage repository \u00b6 Central Artifact storage repository stores all the artifacts related to experiment. The data versioning framework (DVC) stores the artifacts in a content addressable layout. The artifacts are stored inside the folder with name as the first two characters of the content hash and the name of the artifact as the remaining part of the content hash. This helps in efficient retrieval of the artifacts. Git Repository \u00b6 Git repository is used to track the code. Along with the code, the metadata file of the artifacts which contain the content hash of the artifacts are also stored in GIT. The Data versioning framework (dvc) would use these files to retrieve the artifacts from the artifact storage repository.","title":"Components"},{"location":"architecture/components/#cmf-components","text":"Common metadata framework has the following components: Metadata Library exposes API\u2019s to track the pipeline metadata. It also provides API\u2019s to query the stored metadata. Local Client interacts with the server to pull or push metadata from or to the remote store. Central Server interacts with all the remote clients and is responsible to merge the metadata transferred by the remote client and manage the consolidated metadata. Central Repositories hosts the code, data and metadata.","title":"CMF Components"},{"location":"architecture/components/#metadata-library","text":"The API\u2019s and the abstractions provided by the library enables tracking of pipeline metadata. It tracks the stages in the pipeline, the input and output artifacts at each stage and metrics. The framework allows metrics to be tracked both at coarse and fine-grained intervals. It could be a stage metrics, which could be captured at the end of a stage or fine-grained metrics which is tracked per step (epoch) or at regular intervals during the execution of the stage. The metadata logged through the APIs are written to a backend relational database. The library also provides API\u2019s to query the metadata stored in the relational database for the users to inspect pipelines. In addition to explicit tracking through the API\u2019s library also provides, implicit tracking. The implicit tracking automatically tracks the software version used in the pipelines. The function arguments and function return values can be automatically tracked by adding metadata tracker class decorators on the functions. Before writing the metadata to relational database, the metadata operations are journaled in the metadata journal log. This enables the framework to transfer the local metadata to the central server. All artifacts are versioned with a data versioning framework (for e.g., DVC). The content hash of the artifacts are generated and stored along with the user provided metadata. A special artifact metadata file called a \u201c.dvc\u201d file is created for every artifact (file / folder) which is added to data version management system. The .dvc file contains the content hash of the artifact. For every new execution, the metadata tracker creates a new branch to track the code. The special metadata file created for artifacts, the \u201c.dvc\u201d file is also committed to GIT and its commit id is tracked as a metadata information. The artifacts are versioned through the versioning of its metadata file. Whenever there is a change in the artifact, the metadata file is modified to reflect its current content hash, and the file is tracked as a new version of the metadata file. The metadata tracker automatically tracks the start commit when the library was initialized and creates separate commit for each change in the artifact along the experiment. This helps to track the transformations on the artifacts along the different stages in the pipeline.","title":"Metadata Library"},{"location":"architecture/components/#local-client","text":"The metadata client interacts with the metadata server. It communicates with the server, for synchronization of metadata. After the experiment is completed, the user invokes the \u201cCmf push\u201d command to push the collected metadata to the remote. This transfers the existing metadata journal to the server. The metadata from the central repository can be pulled to the local repository, either using the artifacts or using the project as the identifier or both. When artifact is used as the identifier, all metadata associated with the artifacts currently present in the branch of the cloned Git repository is pulled from the central repository to the local repository. The pulled metadata consist of not only the immediate metadata associated with the artifacts, it contains the metadata of all the artifacts in its chain of lineage. When project is used as the identifier, all the metadata associated with the current branch of the pipeline code that is checked out is pulled to the local repository.","title":"Local Client"},{"location":"architecture/components/#central-server","text":"The central server, exposes REST API\u2019s that can be called from the remote clients. This can help in situations where the connectivity between the core datacenter and the remote client is robust. The remote client calls the API\u2019s exposed by the central server to log the metadata directly to the central metadata repository. Where the connectivity with the central server is intermittent, the remote clients log the metadata to the local repository. The journaled metadata is pushed by the remote client to the central server. The central server, will replay the journal and merge the incoming metadata with the metadata already existing in the central repository. The ability to accurately identify the artifacts anywhere using their content hash, makes this merge robust.","title":"Central Server"},{"location":"architecture/components/#central-repositories","text":"The common metadata framework consist of three central repositories for the code, data and metadata.","title":"Central Repositories"},{"location":"architecture/components/#central-metadata-repository","text":"Central metadata repository holds the metadata pushed from the distributed sites. It holds metadata about all the different pipelines that was tracked using the common metadata tracker. The consolidated view of the metadata stored in the central repository, helps the users to learn across various stages in the pipeline executed at different locations. Using the query layer that is pointed to the central repository, the users gets the global view of the metadata which provides them with a deeper understanding of the pipelines and its metadata. The metadata helps to understand nonobvious results like performance of a dataset with respect to other datasets, Performance of a particular pipeline with respect to other pipelines etc.","title":"Central Metadata repository"},{"location":"architecture/components/#central-artifact-storage-repository","text":"Central Artifact storage repository stores all the artifacts related to experiment. The data versioning framework (DVC) stores the artifacts in a content addressable layout. The artifacts are stored inside the folder with name as the first two characters of the content hash and the name of the artifact as the remaining part of the content hash. This helps in efficient retrieval of the artifacts.","title":"Central Artifact storage repository"},{"location":"architecture/components/#git-repository","text":"Git repository is used to track the code. Along with the code, the metadata file of the artifacts which contain the content hash of the artifacts are also stored in GIT. The Data versioning framework (dvc) would use these files to retrieve the artifacts from the artifact storage repository.","title":"Git Repository"},{"location":"architecture/overview/","text":"Architecture Overview \u00b6 Interactions in data pipelines can be complex. The Different stages in the pipeline, (which may not be next to each other) may have to interact to produce or transform artifacts. As the artifacts navigates and undergo transformations through this pipeline, it can take a complicated path, which might also involve bidirectional movement across these stages. Also, there could be dependencies between the multiple stages, where the metrics produced by a stage could influence the metrics at a subsequent stage. It is important to track the metadata across a pipeline to provide features like, lineage tracking, provenance and reproducibility. The tracking of metadata through these complex pipelines have multiple challenges, some of them being, Each stage in the pipeline could be executed in a different datacenter or an edge site having intermittent connection to the core datacenter. Each stage in the pipeline could be possibly managed by different teams. The artifacts (input or output) needs to be uniquely identified across different sites and across multiple pipelines. Common metadata framework (CMF) addresses the problems associated with tracking of pipeline metadata from distributed sites and tracks code, data and metadata together for end-to-end traceability. The framework automatically tracks the code version as one of the metadata for an execution. Additionally, the data artifacts are also versioned automatically using a data versioning framework (like DVC) and the metadata regarding the data version is stored along with the code. The framework stores the Git commit id of the metadata file associated with the artifact and content hash of the artifact as metadata. The framework provides API\u2019s to track the hyperparameters and other metadata of pipelines. Therefore, from the metadata stored, users can zero in on the hyperparameters, code version and the artifact version used for the experiment. Identifying the artifacts by content hash allows the framework, to uniquely identify an artifact anywhere in the distributed sites. This enables the metadata from the distributed sites to be precisely merged to a central repository, thereby providing a single global metadata from the distributed sites. On this backbone, we build the Git like experience for metadata, enabling users to push their local metadata to the remote repository, where it is merged to create the global metadata and pull metadata from the global metadata to the local, to create a local view, which would contain only the metadata of interest. The framework can be used to track various types of pipelines such as data pipelines or AI pipelines.","title":"Overview"},{"location":"architecture/overview/#architecture-overview","text":"Interactions in data pipelines can be complex. The Different stages in the pipeline, (which may not be next to each other) may have to interact to produce or transform artifacts. As the artifacts navigates and undergo transformations through this pipeline, it can take a complicated path, which might also involve bidirectional movement across these stages. Also, there could be dependencies between the multiple stages, where the metrics produced by a stage could influence the metrics at a subsequent stage. It is important to track the metadata across a pipeline to provide features like, lineage tracking, provenance and reproducibility. The tracking of metadata through these complex pipelines have multiple challenges, some of them being, Each stage in the pipeline could be executed in a different datacenter or an edge site having intermittent connection to the core datacenter. Each stage in the pipeline could be possibly managed by different teams. The artifacts (input or output) needs to be uniquely identified across different sites and across multiple pipelines. Common metadata framework (CMF) addresses the problems associated with tracking of pipeline metadata from distributed sites and tracks code, data and metadata together for end-to-end traceability. The framework automatically tracks the code version as one of the metadata for an execution. Additionally, the data artifacts are also versioned automatically using a data versioning framework (like DVC) and the metadata regarding the data version is stored along with the code. The framework stores the Git commit id of the metadata file associated with the artifact and content hash of the artifact as metadata. The framework provides API\u2019s to track the hyperparameters and other metadata of pipelines. Therefore, from the metadata stored, users can zero in on the hyperparameters, code version and the artifact version used for the experiment. Identifying the artifacts by content hash allows the framework, to uniquely identify an artifact anywhere in the distributed sites. This enables the metadata from the distributed sites to be precisely merged to a central repository, thereby providing a single global metadata from the distributed sites. On this backbone, we build the Git like experience for metadata, enabling users to push their local metadata to the remote repository, where it is merged to create the global metadata and pull metadata from the global metadata to the local, to create a local view, which would contain only the metadata of interest. The framework can be used to track various types of pipelines such as data pipelines or AI pipelines.","title":"Architecture Overview"},{"location":"examples/getting_started/","text":"Getting Started \u00b6 This example depends on the following packages: git . We also recommend installing anaconda to manage python virtual environments. This example was tested in the following environments: Ubuntu-22.04 with python-3.8.15 This example demonstrates how CMF tracks a metadata associated with executions of various machine learning (ML) pipelines. ML pipelines differ from other pipelines (e.g., data Extract-Transform-Load pipelines) by the presence of ML steps, such as training and testing ML models. More comprehensive ML pipelines may include steps such as deploying a trained model and tracking its inference parameters (such as response latency, memory consumption etc.). This example, located here implements a simple pipeline consisting of four steps: The parse step splits the raw data into train and test raw datasets for training and testing a machine learning model. This step registers one input artifact (raw dataset ) and two output artifacts (train and test datasets ). The featurize step creates two machine learning splits - train and test splits - that will be used by an ML training algorithm to train ML models. This step registers two input artifacts (raw train and test datasets) and two output artifacts ( train and test ML datasets). The next train step trains an ML model (random forest classifier). It registers one input artifact (train ML dataset) and one output artifact (trained ML model). The fourth test step tests the ML model trained in the third train step. This step registers two input artifacts (ML model and test dataset) and one output artifact (performance metrics). The last query step is a demonstration that shows how pipeline metadata can be retrieved from CMF. It will print metadata associated with all executions of the above steps. This means that if you rerun the pipeline again, the output will include not only metadata associated with the last run, but also metadata associated with all previous runs. Pre-requisites \u00b6 We start by creating (1) a workspace directory that will contain all files for this example and (2) a python virtual environment. Then we will clone the CMF project that contains this example project. # Create workspace directory mkdir cmf_getting_started_example cd cmf_getting_started_example # Create and activate Python virtual environment (the Python version may need to be adjusted depending on your system) conda create -n cmf_getting_started_example python = 3 .8 conda activate cmf_getting_started_example # Clone the CMF project from GitHub and install CMF git clone https://github.com/HewlettPackard/cmf pip install ./cmf Project initialization \u00b6 We need to copy the source tree of the example in its own directory (that must be outside the CMF source tree), and initialize git and dvc for this project. # Create a separate copy of the example project cp -r ./cmf/examples/example-get-started/ ./example-get-started cd ./example-get-started Review the content of the sample_env file which is located in the root directory of the example. For the demonstration purposes, you can leave all fields as is. Once this file is reviewed, source that file and run initialize.sh to initialize git and dvc repositories. # Export environmental variables source ./sample_env # Initialize the example project sh ./initialize.sh Project execution \u00b6 The initialize.sh script executed above has printed some details about the project. To execute the example pipeline, run the test_script.sh file (before that, study the contents of that file). Basically, that script will run a sequence of steps common for a typical machine learning project - getting raw data, converting it into machine learning train/test splits, training and testing a model. The execution of these steps (and parent pipeline) will be recorded by the CMF. # Run the example pipeline sh ./test_script.sh This script will run the pipeline and will store its metadata in a sqlite file named mlmd. Verify that all stages are done using git log command. You should see commits corresponding to the artifacts that were created. Under normal conditions, the next steps would be to: (1) execute the dvc push command to push the artifacts to dvc remote and (2) execute the git push origin command to track the metadata of the generated artifacts. Query \u00b6 The stored metadata can be explored using the query layer. Example Jupyter notebook Query_Tester-base_mlmd.ipynb can be found in this directory. Clean Up \u00b6 Metadata is stored in sqlite file named \"mlmd\". To clean up, delete the \"mlmd\" file. Steps to test dataslice \u00b6 Run the following command: python test-data-slice.py .","title":"Getting Started"},{"location":"examples/getting_started/#getting-started","text":"This example depends on the following packages: git . We also recommend installing anaconda to manage python virtual environments. This example was tested in the following environments: Ubuntu-22.04 with python-3.8.15 This example demonstrates how CMF tracks a metadata associated with executions of various machine learning (ML) pipelines. ML pipelines differ from other pipelines (e.g., data Extract-Transform-Load pipelines) by the presence of ML steps, such as training and testing ML models. More comprehensive ML pipelines may include steps such as deploying a trained model and tracking its inference parameters (such as response latency, memory consumption etc.). This example, located here implements a simple pipeline consisting of four steps: The parse step splits the raw data into train and test raw datasets for training and testing a machine learning model. This step registers one input artifact (raw dataset ) and two output artifacts (train and test datasets ). The featurize step creates two machine learning splits - train and test splits - that will be used by an ML training algorithm to train ML models. This step registers two input artifacts (raw train and test datasets) and two output artifacts ( train and test ML datasets). The next train step trains an ML model (random forest classifier). It registers one input artifact (train ML dataset) and one output artifact (trained ML model). The fourth test step tests the ML model trained in the third train step. This step registers two input artifacts (ML model and test dataset) and one output artifact (performance metrics). The last query step is a demonstration that shows how pipeline metadata can be retrieved from CMF. It will print metadata associated with all executions of the above steps. This means that if you rerun the pipeline again, the output will include not only metadata associated with the last run, but also metadata associated with all previous runs.","title":"Getting Started"},{"location":"examples/getting_started/#pre-requisites","text":"We start by creating (1) a workspace directory that will contain all files for this example and (2) a python virtual environment. Then we will clone the CMF project that contains this example project. # Create workspace directory mkdir cmf_getting_started_example cd cmf_getting_started_example # Create and activate Python virtual environment (the Python version may need to be adjusted depending on your system) conda create -n cmf_getting_started_example python = 3 .8 conda activate cmf_getting_started_example # Clone the CMF project from GitHub and install CMF git clone https://github.com/HewlettPackard/cmf pip install ./cmf","title":"Pre-requisites"},{"location":"examples/getting_started/#project-initialization","text":"We need to copy the source tree of the example in its own directory (that must be outside the CMF source tree), and initialize git and dvc for this project. # Create a separate copy of the example project cp -r ./cmf/examples/example-get-started/ ./example-get-started cd ./example-get-started Review the content of the sample_env file which is located in the root directory of the example. For the demonstration purposes, you can leave all fields as is. Once this file is reviewed, source that file and run initialize.sh to initialize git and dvc repositories. # Export environmental variables source ./sample_env # Initialize the example project sh ./initialize.sh","title":"Project initialization"},{"location":"examples/getting_started/#project-execution","text":"The initialize.sh script executed above has printed some details about the project. To execute the example pipeline, run the test_script.sh file (before that, study the contents of that file). Basically, that script will run a sequence of steps common for a typical machine learning project - getting raw data, converting it into machine learning train/test splits, training and testing a model. The execution of these steps (and parent pipeline) will be recorded by the CMF. # Run the example pipeline sh ./test_script.sh This script will run the pipeline and will store its metadata in a sqlite file named mlmd. Verify that all stages are done using git log command. You should see commits corresponding to the artifacts that were created. Under normal conditions, the next steps would be to: (1) execute the dvc push command to push the artifacts to dvc remote and (2) execute the git push origin command to track the metadata of the generated artifacts.","title":"Project execution"},{"location":"examples/getting_started/#query","text":"The stored metadata can be explored using the query layer. Example Jupyter notebook Query_Tester-base_mlmd.ipynb can be found in this directory.","title":"Query"},{"location":"examples/getting_started/#clean-up","text":"Metadata is stored in sqlite file named \"mlmd\". To clean up, delete the \"mlmd\" file.","title":"Clean Up"},{"location":"examples/getting_started/#steps-to-test-dataslice","text":"Run the following command: python test-data-slice.py .","title":"Steps to test dataslice"}]}