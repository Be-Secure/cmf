{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CMF in a nutshell","text":"<p>CMF (Common Metadata Framework) collects and stores information associated with Machine Learning (ML) pipelines. It  also implements APIs to query this metadata. The CMF adopts a data-first approach: all artifacts (such as datasets, ML models and performance metrics) recorded by the framework are versioned and identified by their content hash.</p>"},{"location":"#installation","title":"Installation","text":"<p>CMF requires 3.8 &gt;= Python &lt;= 3.9. Create python virtual environment:</p> CondaVirtualEnv <pre><code>conda create -n cmf python=3.8\nconda activate cmf\n</code></pre> <pre><code>virtualenv --python=3.8 .cmf\nsource .cmf/bin/activate\n</code></pre> <p>Install CMF</p> Latest version form GitHubStable version form PyPI <pre><code>pip install https://github.com/HewlettPackard/cmf\n</code></pre> <pre><code># Work in progress: not available yet.\n# pip install cmflib\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<ol> <li>Create working directory <code>mkdir &lt;workdir&gt;</code></li> <li>Execute <code>cmf init</code> to configure dvc remote directory, git remote url and cmf server.</li> <li>To configure neo4j backend with cmf export the following environment variables with appropriate values,    <pre><code>export NEO4J_URI=\"bolt://ip:port\"\nexport NEO4J_USER_NAME=&lt;user&gt;\nexport NEO4J_PASSWD=&lt;passwd.\n</code></pre></li> </ol>"},{"location":"#jupyter-lab-docker-container-with-cmf-pre-installed","title":"Jupyter Lab docker container with CMF pre-installed","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Complex ML projects rely on <code>ML pipelines</code> to train and test ML models. An ML pipeline is a sequence of stages where each stage performs a particular task, such as data loading,  pre-processing, ML model training and testing stages. Each stage can have multiple Executions.  Each Execution,</p> <ul> <li>consume <code>inputs</code> and produce <code>outputs</code>.</li> <li>are parametrized by parameters that guide the process of producing outputs.</li> </ul> <p></p> <p>CMF uses the abstractions of <code>Pipeline</code>,<code>Context</code> and <code>Executions</code> to store the metadata of complex ML pipelines. Each pipeline has a name. Users provide it when they initialize the CMF. Each stage is represented by a <code>Context</code> object. Metadata associated with each run of a stage is captured in the Execution object. Inputs and outputs of Executions can be logged as dataset, model or metrics. While parameters of executions are recorded as properties of executions.</p> <p></p> 1 Init2 Stage type3 New execution4 Log Artifacts <p>Start tracking the pipeline metadata by initializing the CMF runtime. The metadata will be associated with the  pipeline named <code>test_pipeline</code>. <pre><code>from cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n\ncmf = Cmf(\n    filename=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n) \n</code></pre></p> <p>Before we can start tracking metadata, we need to let CMF know about stage type. This is not yet associated with  this particular execution. <pre><code>context: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"train\"\n)\n</code></pre></p> <p>Now we can create a new stage execution associated with the <code>train</code> stage. The CMF always creates a new execution, and will adjust its name, so it's unique. This is also the place where we can log execution <code>parameters</code> like seed, hyper-parameters etc . <pre><code>execution: mlmd.proto.Execution = cmf.create_execution(\n    execution_type=\"train\",\n    custom_properties = {\"num_epochs\": 100, \"learning_rate\": 0.01}\n)\n</code></pre></p> <p>Finally, we can log an input (train dataset), and once trained, an output (ML model) artifacts. <pre><code>cmf.log_dataset(\n    'artifacts/test_dataset.csv',   # Dataset path \n    \"input\"                         # This is INPUT artifact\n)\ncmf.log_model(\n    \"artifacts/model.pkl\",          # Model path \n    event=\"output\"                  # This is OUTPUT artifact\n)\n</code></pre></p>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Simple \"getting started\" example is described here. </p>"},{"location":"#api-overview","title":"API Overview","text":"<p>Import CMF. <pre><code>from cmflib import cmf\n</code></pre></p> <p>Initialize CMF. The CMF object is responsible for managing a CMF backend to record  the pipeline metadata. Internally, it creates a pipeline abstraction that groups individual stages and their executions.  All stages, their executions and produced artifacts will be associated with a pipeline with the given name. <pre><code>cmf = cmf.Cmf(\n   filename=\"mlmd\",                # Path to ML Metadata file.\n   pipeline_name=\"mnist\"           # Name of a ML pipeline.\n)                                                       \n</code></pre></p> <p>Define a stage. An ML pipeline can have multiple stages, and each stage can be associated with multiple executions. A stage is like a class in the world of object-oriented programming languages. A context (stage description) defines  what this stage looks like (name and optional properties), and is created with the  create_context method. <pre><code>context = cmf.create_context(\n    pipeline_stage=\"download\",     # Stage name\n    custom_properties={            # Optional properties\n        \"uses_network\": True,      #  Downloads from the Internet\n        \"disk_space\": \"10GB\"       #  Needs this much space\n    }\n)\n</code></pre></p> <p>Create a stage execution. A stage in ML pipeline can have multiple executions. Every run is marked as an execution.  This API helps to track the metadata associated with the execution, like stage parameters (e.g., number of epochs and  learning rate for train stages). The stage execution name does not need to be the same as the name of its context. Moreover, the CMF will adjust this name to ensure every execution has a unique name. The CMF will internally associate this execution with the context created previously. Stage executions are created by calling the  create_execution method. <pre><code>execution = cmf.create_execution(\n    execution_type=\"download\",            # Execution name.\n    custom_properties = {                 # Execution parameters\n        \"url\": \"https://a.com/mnist.gz\"   #  Data URL.\n    }\n)\n</code></pre></p> <p>Log artifacts. A stage execution can consume (inputs) and produce (outputs) multiple artifacts (datasets, models and  performance metrics). The path of these artifacts must be relative to the project (repository) root path. Artifacts  might have optional metadata associated with them. These metadata could include feature statistics for ML datasets, or useful parameters for ML models (such as, for instance, number of trees in a random forest classifier). </p> <ul> <li> <p>Datasets are logged with the log_dataset method.     <pre><code>cmf.log_dataset('data/mnist.gz', \"input\", custom_properties={\"name\": \"mnist\", \"type\": 'raw'})\ncmf.log_dataset('data/train.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"train_split\"})\ncmf.log_dataset('data/test.csv', \"output\", custom_properties={\"name\": \"mnist\", \"type\": \"test_split\"})\n</code></pre></p> </li> <li> <p>ML models produced by training stages are logged using log_model API. ML models can be    both input and output artifacts. The metadata associated with the artifact could be logged as an optional argument.     <pre><code># In train stage\ncmf.log_model(\n   path=\"model/rf.pkl\", event=\"output\", model_framework=\"scikit-learn\", model_type=\"RandomForestClassifier\", \n   model_name=\"RandomForestClassifier:default\" \n)\n\n# In test stage\ncmf.log_model(\n   path=\"model/rf.pkl\", event=\"input\" \n)\n</code></pre></p> </li> <li> <p>Metrics of every optimization step (one epoch of Stochastic Gradient Descent, or one boosting round in    Gradient Boosting Trees) are logged using log_metric API.     <pre><code>#Can be called at every epoch or every step in the training. This is logged to a parquet file and committed at the \n# commit stage.\n\n#Inside training loop\nwhile True: \n     cmf.log_metric(\"training_metrics\", {\"loss\": loss}) \ncmf.commit_metrics(\"training_metrics\")\n</code></pre></p> </li> <li> <p>Stage metrics, or final metrics, are logged with the log_execution_metrics   method. These are final metrics of a stage, such as final train or test accuracy.      <pre><code>cmf.log_execution_metrics(\"metrics\", {\"avg_prec\": avg_prec, \"roc_auc\": roc_auc})\n</code></pre></p> </li> </ul> <p>Dataslices are intended to be used to track subsets of the data. For instance, this can be used to track and compare accuracies of ML models on these subsets to identify model bias. Data slices are created with  the create_dataslice method. <pre><code>dataslice = cmf.create_dataslice(\"slice-a\")\nfor i in range(1, 20, 1):\n    j = random.randrange(100)\n    dataslice.add_data(\"data/raw_data/\"+str(j)+\".xml\")\ndataslice.commit()\n</code></pre></p>"},{"location":"#graph-layer-overview","title":"Graph Layer Overview","text":"<p>CMF library has an optional <code>graph layer</code> which stores the relationships in a Neo4J graph database. To use the graph  layer, the <code>graph</code> parameter in the library init call must be set to true (it is set to false by default). The  library reads the configuration parameters of the graph database from the following environment variables: <code>NEO4J_URI</code>,  <code>NEO4J_USER_NAME</code> and <code>NEO4J_PASSWD</code>. They need to be made available in a user environment, e.g.:</p> <pre><code>export NEO4J_URI=\"bolt://10.93.244.219:7687\"\nexport NEO4J_USER_NAME=neo4j\nexport NEO4J_PASSWD=neo4j </code></pre> <p>To use the graph layer, instantiate the CMF with <code>graph=True</code> parameter:  <pre><code>from cmflib import cmf\n\ncmf =  cmf.Cmf(\n   filename=\"mlmd\",\n   pipeline_name=\"anomaly_detection_pipeline\", \n   graph=True\n)\n</code></pre></p>"},{"location":"#use-a-jupyterlab-docker-environment-with-cmf-pre-installed","title":"Use a Jupyterlab Docker environment with CMF pre-installed","text":"<p>CMF has a docker-compose file which creates two docker containers, - JupyterLab Notebook Environment with CMF pre installed.     - Accessible at http://[HOST.IP.AD.DR]:8888 (default token: <code>docker</code>)     - Within the Jupyterlab environment, a startup script switches context to <code>$USER:$GROUP</code> as specified in <code>.env</code>     - <code>example-get-started</code> from this repo is bind mounted into <code>/home/jovyan/example-get-started</code> - Neo4j Docker container to store and access lineages.</p>"},{"location":"#step-1","title":"Step 1.","text":"<p><code>create .env file in current folder using env-example as a template. Modify the .env file for the following variables USER,UID,GROUP,GID,GIT_USER_NAME,GIT_USER_EMAIL,GIT_REMOTE_URL #These are used by docker-compose.yml</code> </p>"},{"location":"#step-2","title":"Step 2.","text":"<p>Update <code>docker-compose.yml</code> as needed.     your .ssh folder is mounted inside the docker conatiner to enable you to push and pull code from git  To-Do      Create these directories in your home folder <pre><code>mkdir $HOME/workspace \nmkdir $HOME/dvc_remote \n</code></pre> workspace - workspace will be mounted inside the cmf pre-installed docker conatiner (can be your code directory)   dvc_remote - remote data store for dvc </p> <p>or Change the below lines in docker-compose to reflect the appropriate directories <pre><code> If your workspace is named \"experiment\" change the below line\n$HOME/workspace:/home/jovyan/workspace to \n$HOME/experiment:/home/jovyan/wokspace\n</code></pre> <pre><code>If your remote is /extmount/data change the line \n$HOME/dvc_remote:/home/jovyan/dvc_remote to \n/extmount/data:/home/jovyan/dvc_remote \n</code></pre> Start the docker <pre><code>docker-compose up --build -d\n</code></pre> Access the jupyter notebook http://[HOST.IP.AD.DR]:8888 (default token: <code>docker</code>)</p> <p>Click the terminal icon Quick Start <pre><code>cd example-get-started\nsh initialize.sh\nsh test_script.sh\ndvc push\n</code></pre> The above steps will run a pre coded example pipeline and the metadata is stored in a file named \"mlmd\". The artifacts created will be pushed to configured dvc remote (default: /home/dvc_remote) The stored metadata is displayed as  </p> <p>Metadata lineage can be accessed in neo4j. Open http://host:7475/browser/ Connect to server with default password neo4j123 (To change this modify .env file)  Run the query  <pre><code>MATCH (a:Execution)-[r]-(b) WHERE (b:Dataset or b:Model or b:Metrics) RETURN a,r, b     \n</code></pre> Expected output </p> <p>Jupyter Lab Notebook  Select the kernel as Python[conda env:python37] </p> <p>Shutdown/remove (Remove volumes as well) <pre><code>docker-compose down -v\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>CMF is an open source project hosted on GitHub and distributed according to the Apache 2.0 licence. We are welcome user contributions - send us a message on the Slack channel or open a GitHub  issue or a pull request  on GitHub.</p>"},{"location":"#citation","title":"Citation","text":"<pre><code>@mist{foltin2022cmf,\ntitle={Self-Learning Data Foundation for Scientific AI},\nauthor={Martin Foltin, Annmary Justine, Sergey Serebryakov, Cong Xu, Aalap Tripathy, Suparna Bhattacharya, \n            Paolo Faraboschi},\nyear={2022},\nnote = {Presented at the \"Monterey Data Conference\"},\nURL={https://drive.google.com/file/d/1Oqs0AN0RsAjt_y9ZjzYOmBxI8H0yqSpB/view},\n}\n</code></pre>"},{"location":"#community","title":"Community","text":"<p>Help</p> <p>Common Metadata Framework and its documentation are in active stage of development and are very new. If there is anything unclear, missing or there's a typo, please, open an issue or pull request  on GitHub.</p>"},{"location":"_src/","title":"CMF docs development resources","text":"<p>This directory contains files that are used to create some content for the CMF documentation. This process is not automated yet. Files in this directory are not supposed to be referenced from documentation pages.</p> <p>It also should not be required to automatically redeploy documentation (e.g., with GitHub actions) when documentation files change only in this particular directory.</p> <ul> <li>The diagrams.drawio file is created with PyCharm's    Diagram.NET plugin. It contains a number of diagrams used in the documentation. Now,   to update those diagrams, use this file to edit them, them take a screenshot, edit with some editor, and then    overwrite corresponding files (e.g., ML Pipeline Definition) used on the main page.</li> </ul>"},{"location":"api/public/API/","title":"API","text":""},{"location":"api/public/API/#logging-apis","title":"Logging API'S","text":""},{"location":"api/public/API/#1-library-init-call-cmf","title":"1. Library init call - Cmf()","text":"<p>This calls initiates the library and also creates a pipeline object with the name provided. Arguments to be passed CMF:</p> <pre>\ncmf = cmf.Cmf(filename=\"mlmd\", pipeline_name=\"Test-env\")  \n\nReturns a Context object of mlmd.proto.Context\n</pre> Arguments filename String Path  to the sqlite file to store the metadata pipeline_name String Name to uniquely identify the pipeline. Note that name is the unique identification for a pipeline.  If a pipeline already exist with the same name, the existing pipeline object is reused custom_properties Dictionary (Optional Parameter) - Additional properties of the pipeline that needs to be stored graph Bool (Optional Parameter) If set to true, the libray also stores the relationships in the provided graph database.  Following environment variables should be set   NEO4J_URI - The value should be set to the Graph server URI .  export NEO4J_URI=\"bolt://ip:port\"  User name and password  export NEO4J_USER_NAME=neo4j  export NEO4J_PASSWD=neo4j <p>Return Object mlmd.proto.Context </p> mlmd.proto.Context Attributes create_time_since_epoch int64 create_time_since_epoch custom_properties repeated CustomPropertiesEntry custom_properties id int64 id last_update_time_since_epoch int64 last_update_time_since_epoch name string name properties repeated PropertiesEntry properties type string type type_id int64 type_id ### 2. create_context - Creates a Stage with properties A pipeline may include multiple stages. A unique name should be provided for every Stage in a pipeline. Arguments to be passed CMF: <pre>\ncontext = cmf.create_context(pipeline_stage=\"Prepare\", custom_properties ={\"user-metadata1\":\"metadata_value\"}\n</pre> Arguments pipeline_stage String Name of the pipeline Stage custom_properties Dictionary (Optional Parameter) - The developer's can provide key value pairs of additional properties of the stage that needs to be stored. <p>Return Object mlmd.proto.Context  |mlmd.proto.Context  Attributes| | |------|------| |create_time_since_epoch|   int64 create_time_since_epoch| |custom_properties| repeated CustomPropertiesEntry custom_properties| |id|    int64 id| |last_update_time_since_epoch|  int64 last_update_time_since_epoch| |name|  string name| |properties|    repeated PropertiesEntry properties| |type|  string type| |type_id|   int64 type_id|</p>"},{"location":"api/public/API/#3-create_execution-creates-an-execution-with-properties","title":"3. create_execution - Creates an Execution with properties","text":"<p>A stage can have multiple executions. A unique name should ne provided for exery execution.  Properties of the execution can be paased as key value pairs in the custom properties. Eg: The hyper parameters used for the execution can be passed.</p> <pre>\n\nexecution = cmf.create_execution(execution_type=\"Prepare\",\n                                              custom_properties = {\"Split\":split, \"Seed\":seed})\nexecution_type:String - Name of the execution\ncustom_properties:Dictionary (Optional Parameter)\nReturn Execution object of type mlmd.proto.Execution\n</pre> Arguments execution_type String Name of the execution custom_properties Dictionary (Optional Parameter) <p>Return object of type mlmd.proto.Execution | mlmd.proto.Execution Attributes|                | |---------------|-------------| |create_time_since_epoch    |int64 create_time_since_epoch| |custom_properties  |repeated CustomPropertiesEntry custom_properties| |id |int64 id| |last_known_state   |State last_known_state| |last_update_time_since_epoch|  int64 last_update_time_since_epoch| |name   |string name| |properties |repeated PropertiesEntry properties [Git_Repo, Context_Type, Git_Start_Commit, Pipeline_Type, Context_ID, Git_End_Commit, Execution(Command used), Pipeline_id| |type   |string type| |type_id|   int64 type_id|</p>"},{"location":"api/public/API/#4-log_dataset-logs-a-dataset-and-its-properties","title":"4. log_dataset - Logs a Dataset and its properties","text":"<p>Tracks a Dataset and its version. The version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata. </p> <pre>\nartifact = cmf.log_dataset(\"/repo/data.xml\", \"input\", custom_properties={\"Source\":\"kaggle\"})\n</pre> Arguments url String The path to the dataset event String Takes arguments INPUT OR OUTPUT custom_properties Dictionary The Dataset properties <p>Returns an Artifact object of type mlmd.proto.Artifact</p> mlmd.proto.Artifact Attributes create_time_since_epoch int64 create_time_since_epoch custom_properties repeated CustomPropertiesEntry custom_properties id int64 id last_update_time_since_epoch int64 last_update_time_since_epoch name string name properties repeated PropertiesEntry properties(Commit, Git_Repo) state State state type string type type_id int64 type_id uri string uri"},{"location":"api/public/API/#5-log_model-logs-a-model-and-its-properties","title":"5. log_model - Logs a model and its properties.","text":"<pre>\ncmf.log_model(path=\"path/to/model.pkl\", event=\"output\", model_framework=\"SKlearn\", model_type=\"RandomForestClassifier\", model_name=\"RandomForestClassifier:default\")\n\nReturns an Artifact object of type mlmd.proto.Artifact\n</pre> Arguments path String Path to the model model file event String Takes arguments INPUT OR OUTPUT model_framework String Framework used to create model model_type String Type of Model Algorithm used model_name String Name of the Algorithm used custom_properties Dictionary The model properties <p>Returns Atifact object of type mlmd.proto.Artifact |mlmd.proto.Artifact Attributes| | |-----------|---------| |create_time_since_epoch|   int64 create_time_since_epoch| |custom_properties| repeated CustomPropertiesEntry custom_properties| |id|    int64 id |last_update_time_since_epoch|  int64 last_update_time_since_epoch |name|  string name |properties|    repeated PropertiesEntry properties(commit, model_framework, model_type, model_name)| |state| State state| |type|  string type| |type_id|   int64 type_id| |uri|   string uri|</p>"},{"location":"api/public/API/#6-log_execution_metrics-logs-the-metrics-for-the-execution","title":"6. log_execution_metrics Logs the metrics for the execution","text":"<pre>\ncmf.log_execution_metrics(metrics_name :\"Training_Metrics\", {\"auc\":auc,\"loss\":loss}\n</pre> Arguments metrics_name String Name to identify the metrics custom_properties Dictionary Metrics"},{"location":"api/public/API/#7-log_metrics-logs-the-per-step-metrics-for-fine-grained-tracking","title":"7. log_metrics Logs the per Step metrics for fine grained tracking","text":"<p>The metrics provided is stored in a parquet file. The commit_metrics call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the read_metrics call</p> <pre>\n#Can be called at every epoch or every step in the training. This is logged to a parquet file and commited at the commit stage.\nwhile True: #Inside training loop\n     metawriter.log_metric(\"training_metrics\", {\"loss\":loss}) \nmetawriter.commit_metrics(\"training_metrics\")\n</pre> Arguments for log_metric metrics_name String Name to identify the metrics custom_properties Dictionary Metrics Arguments for commit_metrics metrics_name String Name to identify the metrics"},{"location":"api/public/API/#8-create_dataslice","title":"8. create_dataslice","text":"<p>This helps to track a subset of the data. Currently supported only for file abstractions.  For eg- Accuracy of the model for a slice of data(gender, ethnicity etc)</p> <pre>\ndataslice = cmf.create_dataslice(\"slice-a\")\n</pre> Arguments for create_dataslice name String Name to identify the dataslice Returns a Dataslice object"},{"location":"api/public/API/#9-add_data-adds-data-to-a-dataslice","title":"9. add_data Adds data to a dataslice.","text":"<p>Currently supported only for file abstractions. Pre condition - The parent folder, containing the file should already be versioned. </p> <pre>\ndataslice.add_data(\"data/raw_data/\"+str(j)+\".xml\")\n</pre> Arguments name String Name to identify the file to be added to the dataslice"},{"location":"api/public/API/#10-dataslice-commit-commits-the-created-dataslice","title":"10. Dataslice Commit - Commits the created dataslice","text":"<p>The created dataslice is versioned and added to underneath data versioning softwarre</p> <pre>\ndataslice.commit()\n</pre>"},{"location":"api/public/cmf/","title":"cmflib.cmf.Cmf","text":"<p>This class provides methods to log metadata for distributed AI pipelines. The class instance creates an ML metadata store to store the metadata. It creates a driver to store nodes and its relationships to neo4j. The user has to provide the name of the pipeline, that needs to be recorded with it. <pre><code>cmflib.cmf.Cmf(\n    filename=\"mlmd\",\n    pipeline_name=\"test_pipeline\",\n    custom_properties={\"owner\": \"user_a\"},\n    graph=False\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path  to the sqlite file to store the metadata</p> <code>'mlmd'</code> <code>pipeline_name</code> <code>str</code> <p>Name to uniquely identify the pipeline. Note that name is the unique identification for a pipeline.  If a pipeline already exist with the same name, the existing pipeline object is reused.</p> <code>''</code> <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Additional properties of the pipeline that needs to be stored.</p> <code>None</code> <code>graph</code> <code>bool</code> <p>If set to true, the libray also stores the relationships in the provided graph database. The following variables should be set: <code>neo4j_uri</code> (graph server URI), <code>neo4j_user</code> (user name) and <code>neo4j_password</code> (user password), e.g.: <pre><code>   cmf init local --path /home/user/local-storage --git-remote-url https://github.com/XXX/exprepo.git --neo4j-user neo4j --neo4j-password neo4j \n                  --neo4j-uri bolt://localhost:7687\n</code></pre></p> <code>False</code> Source code in <code>cmflib/cmf.py</code> <pre><code>def __init__(self, filename: str = \"mlmd\",\n             pipeline_name: str = \"\", custom_properties: t.Optional[t.Dict] = None,\n             graph: bool = False, is_server: bool = False):\n    if is_server is False:\n        Cmf.__prechecks()\n    if custom_properties is None:\n        custom_properties = {}\n    config = mlpb.ConnectionConfig()\n    config.sqlite.filename_uri = filename\n    self.store = metadata_store.MetadataStore(config)\n    self.filename = filename\n    self.child_context = None\n    self.execution = None\n    self.execution_name = \"\"\n    self.execution_command = \"\"\n    self.metrics = {}\n    self.input_artifacts = []\n    self.execution_label_props = {}\n    self.graph = graph\n    self.branch_name = filename.rsplit('/', 1)[-1]\n\n    if is_server is False:\n        git_checkout_new_branch(self.branch_name)\n    self.parent_context = get_or_create_parent_context(\n        store=self.store, pipeline=pipeline_name, custom_properties=custom_properties)\n    if graph is True:\n        self.driver = graph_wrapper.GraphDriver(\n            Cmf.__neo4j_uri, Cmf.__neo4j_user, Cmf.__neo4j_password)\n        self.driver.create_pipeline_node(\n            pipeline_name, self.parent_context.id, custom_properties)\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_context","title":"<code>create_context(pipeline_stage, custom_properties=None)</code>","text":"<p>Create's a  context(stage). Every call creates a unique pipeline stage. </p> Example <p><pre><code>#Create context\n# Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\ncmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create context\ncontext: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n</code></pre> Args:     Pipeline_stage: Name of the Stage.     custom_properties: Developers can provide key value pairs with additional properties of the execution that         need to be stored. Returns:     Context object from ML Metadata library associated with the new context for this stage.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def create_context(self, pipeline_stage: str, custom_properties: t.Optional[t.Dict] = None) -&gt; mlpb.Context:\n\"\"\"Create's a  context(stage).\n    Every call creates a unique pipeline stage. \n    Example:\n        ```python\n        #Create context\n        # Import CMF\n        from cmflib.cmf import Cmf\n        from ml_metadata.proto import metadata_store_pb2 as mlpb\n        # Create CMF logger\n        cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\")\n        # Create context\n        context: mlmd.proto.Context = cmf.create_context(\n            pipeline_stage=\"prepare\",\n            custom_properties ={\"user-metadata1\": \"metadata_value\"}\n        )\n\n        ```\n        Args:\n            Pipeline_stage: Name of the Stage.\n            custom_properties: Developers can provide key value pairs with additional properties of the execution that\n                need to be stored.\n        Returns:\n            Context object from ML Metadata library associated with the new context for this stage.\n    \"\"\"\n    custom_props = {} if custom_properties is None else custom_properties\n    pipeline_stage = self.parent_context.name+'/'+pipeline_stage\n    ctx = get_or_create_run_context(self.store, pipeline_stage, custom_props)\n    self.child_context = ctx\n    associate_child_to_parent_context(store=self.store, parent_context=self.parent_context,\n                                      child_context=ctx)\n    if self.graph:\n        self.driver.create_stage_node(\n            pipeline_stage, self.parent_context, ctx.id, custom_props)\n    return ctx\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_execution","title":"<code>create_execution(execution_type, custom_properties=None, create_new_execution=True)</code>","text":"<p>Create execution. Every call creates a unique execution. Execution can only be created within a context, so create_context must be called first.</p> Example <pre><code># Import CMF\nfrom cmflib.cmf import Cmf\nfrom ml_metadata.proto import metadata_store_pb2 as mlpb\n# Create CMF logger\ncmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\")\n# Create or reuse context for this stage\ncontext: mlmd.proto.Context = cmf.create_context(\n    pipeline_stage=\"prepare\",\n    custom_properties ={\"user-metadata1\": \"metadata_value\"}\n)\n# Create a new execution for this stage run\nexecution: mlmd.proto.Execution = cmf.create_execution(\n    execution_type=\"Prepare\",\n    custom_properties = {\"split\": split, \"seed\": seed}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>execution_type</code> <code>str</code> <p>Type of the execution.(when create_new_execution is False, this is the name of execution)</p> required <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Developers can provide key value pairs with additional properties of the execution that need to be stored.</p> <code>None</code> <code>create_new_execution</code> <code>bool</code> <p>bool = True, This can be used by advanced users to re-use executions This is applicable, when working with framework code like mmdet, pytorch lightning etc, where the  custom call-backs are used to log metrics. if create_new_execution is True(Default), execution_type parameter will be used as the name of the execution type. if create_new_execution is False, if existing execution exist with the same name as execution_type.  it will be reused. Only executions created with  create_new_execution as False will have \"name\" as a property.</p> <code>True</code> <p>Returns:</p> Type Description <code>mlpb.Execution</code> <p>Execution object from ML Metadata library associated with the new execution for this stage.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def create_execution(self, execution_type: str, \n                     custom_properties: t.Optional[t.Dict] = None, create_new_execution:bool = True) -&gt; mlpb.Execution:\n\"\"\"Create execution.\n    Every call creates a unique execution. Execution can only be created within a context, so\n    [create_context][cmflib.cmf.Cmf.create_context] must be called first.\n    Example:\n        ```python\n        # Import CMF\n        from cmflib.cmf import Cmf\n        from ml_metadata.proto import metadata_store_pb2 as mlpb\n        # Create CMF logger\n        cmf = Cmf(filename=\"mlmd\", pipeline_name=\"test_pipeline\")\n        # Create or reuse context for this stage\n        context: mlmd.proto.Context = cmf.create_context(\n            pipeline_stage=\"prepare\",\n            custom_properties ={\"user-metadata1\": \"metadata_value\"}\n        )\n        # Create a new execution for this stage run\n        execution: mlmd.proto.Execution = cmf.create_execution(\n            execution_type=\"Prepare\",\n            custom_properties = {\"split\": split, \"seed\": seed}\n        )\n        ```\n    Args:\n        execution_type: Type of the execution.(when create_new_execution is False, this is the name of execution)\n        custom_properties: Developers can provide key value pairs with additional properties of the execution that\n            need to be stored.\n        create_new_execution:bool = True, This can be used by advanced users to re-use executions\n            This is applicable, when working with framework code like mmdet, pytorch lightning etc, where the \n            custom call-backs are used to log metrics.\n            if create_new_execution is True(Default), execution_type parameter will be used as the name of the execution type.\n            if create_new_execution is False, if existing execution exist with the same name as execution_type. \n            it will be reused.\n            Only executions created with  create_new_execution as False will have \"name\" as a property.\n\n    Returns:\n        Execution object from ML Metadata library associated with the new execution for this stage.\n    \"\"\"\n    # Initializing the execution related fields\n    self.metrics = {}\n    self.input_artifacts = []\n    self.execution_label_props = {}\n    custom_props = {} if custom_properties is None else custom_properties\n    git_repo = git_get_repo()\n    git_start_commit = git_get_commit()\n    self.execution = create_new_execution_in_existing_run_context(\n        store=self.store,\n        execution_type_name=self.child_context.name, # Type field when re-using executions\n        execution_name=execution_type, #Name field if we are re-using executions\n                                       #Type field , if creating new executions always \n        context_id=self.child_context.id,\n        execution=str(sys.argv),\n        pipeline_id=self.parent_context.id,\n        pipeline_type=self.parent_context.name,\n        git_repo=git_repo,\n        git_start_commit=git_start_commit,\n        custom_properties=custom_props,\n        create_new_execution=create_new_execution\n    )\n    self.execution_name = str(self.execution.id) + \",\" + execution_type\n    self.execution_command = str(sys.argv)\n    for k, v in custom_props.items():\n        k = re.sub('-', '_', k)\n        self.execution_label_props[k] = v\n    self.execution_label_props[\"Execution_Name\"] = execution_type + \\\n                                                   \":\" + str(self.execution.id)\n    self.execution_label_props[\"execution_command\"] = str(sys.argv)\n    if self.graph:\n        self.driver.create_execution_node(\n            self.execution_name, self.child_context.id, self.parent_context, str(\n                sys.argv), self.execution.id, custom_props)\n    return self.execution\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_dataset","title":"<code>log_dataset(url, event, custom_properties=None, external=False)</code>","text":"<p>Logs a dataset as artifact. This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata.</p> Example <pre><code>artifact: mlmd.proto.Artifact = cmf.log_dataset(\n    url=\"/repo/data.xml\",\n    event=\"input\",\n    custom_properties={\"source\":\"kaggle\"}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The path to the dataset.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Dataset properties (key/value pairs).</p> <code>None</code> <p>Returns:</p> Type Description <code>mlpb.Artifact</code> <p>Artifact object from ML Metadata library associated with the new dataset artifact.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def log_dataset(self, url: str, event: str, custom_properties: t.Optional[t.Dict] = None, external:bool= False) -&gt; mlpb.Artifact:\n\"\"\"Logs a dataset as artifact.\n    This call adds the dataset to dvc. The dvc metadata file created (.dvc) will be added to git and committed. The\n    version of the  dataset is automatically obtained from the versioning software(DVC) and tracked as a metadata.\n    Example:\n        ```python\n        artifact: mlmd.proto.Artifact = cmf.log_dataset(\n            url=\"/repo/data.xml\",\n            event=\"input\",\n            custom_properties={\"source\":\"kaggle\"}\n        )\n        ```\n    Args:\n         url: The path to the dataset.\n         event: Takes arguments `INPUT` OR `OUTPUT`.\n         custom_properties: Dataset properties (key/value pairs).\n    Returns:\n        Artifact object from ML Metadata library associated with the new dataset artifact.\n    \"\"\"\n    custom_props = {} if custom_properties is None else custom_properties\n    git_repo = git_get_repo()\n    name = re.split('/', url)[-1]\n    event_type = mlpb.Event.Type.OUTPUT\n    existing_artifact = []\n    if event.lower() == \"input\":\n        event_type = mlpb.Event.Type.INPUT\n\n    commit_output(url, self.execution.id)\n    c_hash = dvc_get_hash(url)\n    dataset_commit = c_hash\n    dvc_url = dvc_get_url(url)\n    dvc_url_with_pipeline = f\"{self.parent_context.name}:{dvc_url}\"\n    url = url + \":\" + c_hash\n    if c_hash and c_hash.strip:\n        existing_artifact.extend(self.store.get_artifacts_by_uri(c_hash))\n\n    # To Do - What happens when uri is the same but names are different\n    if existing_artifact and len(existing_artifact) != 0:\n        existing_artifact = existing_artifact[0]\n\n        # Quick fix- Updating only the name\n        if custom_properties is not None:\n            self.update_existing_artifact(\n                existing_artifact, custom_properties)\n        uri = c_hash\n        # update url for existing artifact\n        self.update_dataset_url(existing_artifact, dvc_url_with_pipeline)\n        artifact = link_execution_to_artifact(\n            store=self.store,\n            execution_id=self.execution.id,\n            uri=uri,\n            input_name=url,\n            event_type=event_type)\n    else:\n        # if((existing_artifact and len(existing_artifact )!= 0) and c_hash != \"\"):\n        #   url = url + \":\" + str(self.execution.id)\n        uri = c_hash if c_hash and c_hash.strip() else str(uuid.uuid1())\n        artifact = create_new_artifact_event_and_attribution(\n            store=self.store,\n            execution_id=self.execution.id,\n            context_id=self.child_context.id,\n            uri=uri,\n            name=url,\n            type_name=\"Dataset\",\n            event_type=event_type,\n            properties={\n                \"git_repo\": str(git_repo),\n                \"Commit\": str(dataset_commit),     #passing c_hash value to commit\n                \"url\": str(dvc_url_with_pipeline)},\n            artifact_type_properties={\n                \"git_repo\": mlpb.STRING,\n                \"Commit\": mlpb.STRING,\n                \"url\": mlpb.STRING},\n            custom_properties=custom_props,\n            milliseconds_since_epoch=int(\n                time.time() * 1000),\n        )\n    custom_props[\"git_repo\"] = git_repo\n    custom_props[\"Commit\"] = dataset_commit\n    self.execution_label_props[\"git_repo\"] = git_repo\n    self.execution_label_props[\"Commit\"] = dataset_commit\n\n    if self.graph:\n        self.driver.create_dataset_node(\n            name,\n            url,\n            uri,\n            event,\n            self.execution.id,\n            self.parent_context,\n            custom_props)\n        if event.lower() == \"input\":\n            self.input_artifacts.append({\"Name\": name,\n                                         \"Path\": url,\n                                         \"URI\": uri,\n                                         \"Event\": event.lower(),\n                                         \"Execution_Name\": self.execution_name,\n                                         \"Type\": \"Dataset\",\n                                         \"Execution_Command\": self.execution_command,\n                                         \"Pipeline_Id\": self.parent_context.id,\n                                         \"Pipeline_Name\": self.parent_context.name})\n            self.driver.create_execution_links(uri, name, \"Dataset\")\n        else:\n            child_artifact = {\n                \"Name\": name,\n                \"Path\": url,\n                \"URI\": uri,\n                \"Event\": event.lower(),\n                \"Execution_Name\": self.execution_name,\n                \"Type\": \"Dataset\",\n                \"Execution_Command\": self.execution_command,\n                \"Pipeline_Id\": self.parent_context.id,\n                \"Pipeline_Name\": self.parent_context.name}\n            self.driver.create_artifact_relationships(\n                self.input_artifacts, child_artifact, self.execution_label_props)\n    return artifact\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_model","title":"<code>log_model(path, event, model_framework='Default', model_type='Default', model_name='Default', custom_properties=None)</code>","text":"<p>Logs a model. The model is added to dvc and the metadata file (.dvc) gets committed to git.</p> Example <pre><code>artifact: mlmd.proto.Artifact= cmf.log_model(\n    path=\"path/to/model.pkl\",\n    event=\"output\",\n    model_framework=\"SKlearn\",\n    model_type=\"RandomForestClassifier\",\n    model_name=\"RandomForestClassifier:default\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the model file.</p> required <code>event</code> <code>str</code> <p>Takes arguments <code>INPUT</code> OR <code>OUTPUT</code>.</p> required <code>model_framework</code> <code>str</code> <p>Framework used to create the model.</p> <code>'Default'</code> <code>model_type</code> <code>str</code> <p>Type of model algorithm used.</p> <code>'Default'</code> <code>model_name</code> <code>str</code> <p>Name of the algorithm used.</p> <code>'Default'</code> <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>The model properties.</p> <code>None</code> <p>Returns:</p> Type Description <code>mlpb.Artifact</code> <p>Artifact object from ML Metadata library associated with the new model artifact.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def log_model(self, path: str, event: str, model_framework: str = \"Default\",\n              model_type: str = \"Default\", model_name: str = \"Default\",\n              custom_properties: t.Optional[t.Dict] = None) -&gt; mlpb.Artifact:\n\"\"\"Logs a model.\n    The model is added to dvc and the metadata file (.dvc) gets committed to git.\n    Example:\n        ```python\n        artifact: mlmd.proto.Artifact= cmf.log_model(\n            path=\"path/to/model.pkl\",\n            event=\"output\",\n            model_framework=\"SKlearn\",\n            model_type=\"RandomForestClassifier\",\n            model_name=\"RandomForestClassifier:default\"\n        )\n        ```\n    Args:\n        path: Path to the model file.\n        event: Takes arguments `INPUT` OR `OUTPUT`.\n        model_framework: Framework used to create the model.\n        model_type: Type of model algorithm used.\n        model_name: Name of the algorithm used.\n        custom_properties: The model properties.\n    Returns:\n        Artifact object from ML Metadata library associated with the new model artifact.\n    \"\"\"\n\n    if custom_properties is None:\n        custom_properties = {}\n    custom_props = {} if custom_properties is None else custom_properties\n    # name = re.split('/', path)[-1]\n    event_type = mlpb.Event.Type.OUTPUT\n    existing_artifact = []\n    if event.lower() == \"input\":\n        event_type = mlpb.Event.Type.INPUT\n\n    commit_output(path, self.execution.id)\n    c_hash = dvc_get_hash(path)\n    model_commit = c_hash\n\n    # If connecting to an existing artifact - The name of the artifact is\n    # used as path/steps/key\n    model_uri = path + \":\" + c_hash\n    dvc_url = dvc_get_url(path, False)\n    url = dvc_url\n    url_with_pipeline = f\"{self.parent_context.name}:{url}\"\n    uri = \"\"\n    if c_hash and c_hash.strip():\n        uri = c_hash.strip()\n        existing_artifact.extend(self.store.get_artifacts_by_uri(uri))\n    else:\n        raise RuntimeError(\"Model commit failed, Model uri empty\")\n\n    if existing_artifact and len(\n            existing_artifact) != 0 and event_type == mlpb.Event.Type.INPUT:\n        # update url for existing artifact\n        existing_artifact = self.update_model_url(existing_artifact, url_with_pipeline)\n        artifact = link_execution_to_artifact(\n            store=self.store,\n            execution_id=self.execution.id,\n            uri=c_hash,\n            input_name=model_uri,\n            event_type=event_type)\n        model_uri = artifact.name\n    else:\n\n        uri = c_hash if c_hash and c_hash.strip() else str(uuid.uuid1())\n        model_uri = model_uri + \":\" + str(self.execution.id)\n        artifact = create_new_artifact_event_and_attribution(\n            store=self.store,\n            execution_id=self.execution.id,\n            context_id=self.child_context.id,\n            uri=uri,\n            name=model_uri,\n            type_name=\"Model\",\n            event_type=event_type,\n            properties={\n                \"model_framework\": str(model_framework),\n                \"model_type\": str(model_type),\n                \"model_name\": str(model_name),\n                \"Commit\": str(model_commit),           #passing c_hash value to commit\n                \"url\": str(url_with_pipeline)},\n            artifact_type_properties={\n                \"model_framework\": mlpb.STRING,\n                \"model_type\": mlpb.STRING,\n                \"model_name\": mlpb.STRING,\n                \"Commit\": mlpb.STRING,\n                \"url\": mlpb.STRING,\n            },\n            custom_properties=custom_props,\n            milliseconds_since_epoch=int(\n                time.time() * 1000),\n        )\n    # custom_properties[\"Commit\"] = model_commit\n    self.execution_label_props[\"Commit\"] = model_commit\n    if self.graph:\n        self.driver.create_model_node(\n            model_uri,\n            uri,\n            event,\n            self.execution.id,\n            self.parent_context,\n            custom_props)\n        if event.lower() == \"input\":\n\n            self.input_artifacts.append(\n                {\"Name\": model_uri, \"URI\": uri, \"Event\": event.lower(),\n                 \"Execution_Name\": self.execution_name,\n                 \"Type\": \"Model\", \"Execution_Command\": self.execution_command,\n                 \"Pipeline_Id\": self.parent_context.id,\n                 \"Pipeline_Name\": self.parent_context.name})\n            self.driver.create_execution_links(uri, model_uri, \"Model\")\n        else:\n\n            child_artifact = {\n                \"Name\": model_uri,\n                \"URI\": uri,\n                \"Event\": event.lower(),\n                \"Execution_Name\": self.execution_name,\n                \"Type\": \"Model\",\n                \"Execution_Command\": self.execution_command,\n                \"Pipeline_Id\": self.parent_context.id,\n                \"Pipeline_Name\": self.parent_context.name}\n\n            self.driver.create_artifact_relationships(\n                self.input_artifacts, child_artifact, self.execution_label_props)\n\n    return artifact\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_execution_metrics","title":"<code>log_execution_metrics(metrics_name, custom_properties=None)</code>","text":"<p>Log the metadata associated with the execution (coarse-grained tracking). It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we have.</p> Example <pre><code>exec_metrics: mlpb.Artifact = cmf.log_execution_metrics(\n    metrics_name=\"Training_Metrics\",\n    {\"auc\": auc, \"loss\": loss}\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Dictionary with metric values.</p> <code>None</code> <p>Returns:</p> Type Description <code>mlpb.Artifact</code> <p>Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def log_execution_metrics(self, metrics_name: str, custom_properties: t.Optional[t.Dict] = None) -&gt; mlpb.Artifact:\n\"\"\"Log the metadata associated with the execution (coarse-grained tracking).\n    It is stored as a metrics artifact. This does not have a backing physical file, unlike other artifacts that we\n    have.\n    Example:\n        ```python\n        exec_metrics: mlpb.Artifact = cmf.log_execution_metrics(\n            metrics_name=\"Training_Metrics\",\n            {\"auc\": auc, \"loss\": loss}\n        )\n        ```\n    Args:\n        metrics_name: Name to identify the metrics.\n        custom_properties: Dictionary with metric values.\n    Returns:\n          Artifact object from ML Metadata library associated with the new coarse-grained metrics artifact.\n    \"\"\"\n    custom_props = {} if custom_properties is None else custom_properties\n    uri = str(uuid.uuid1())\n    metrics_name = metrics_name + \":\" + uri + \":\" + str(self.execution.id)\n    metrics = create_new_artifact_event_and_attribution(\n        store=self.store,\n        execution_id=self.execution.id,\n        context_id=self.child_context.id,\n        uri=uri,\n        name=metrics_name,\n        type_name=\"Metrics\",\n        event_type=mlpb.Event.Type.OUTPUT,\n        properties={\n            \"metrics_name\": metrics_name},\n        artifact_type_properties={\n            \"metrics_name\": mlpb.STRING},\n        custom_properties=custom_props,\n        milliseconds_since_epoch=int(\n            time.time() * 1000),\n    )\n    if self.graph:\n        # To do create execution_links\n        self.driver.create_metrics_node(\n            metrics_name,\n            uri,\n            \"output\",\n            self.execution.id,\n            self.parent_context,\n            custom_props)\n        child_artifact = {\n            \"Name\": metrics_name,\n            \"URI\": uri,\n            \"Event\": \"output\",\n            \"Execution_Name\": self.execution_name,\n            \"Type\": \"Metrics\",\n            \"Execution_Command\": self.execution_command,\n            \"Pipeline_Id\": self.parent_context.id,\n            \"Pipeline_Name\": self.parent_context.name}\n        self.driver.create_artifact_relationships(\n            self.input_artifacts, child_artifact, self.execution_label_props)\n    return metrics\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.log_metric","title":"<code>log_metric(metrics_name, custom_properties=None)</code>","text":"<p>Stores the fine-grained (per step or per epoch) metrics to memory. The metrics provided are stored in a parquet file. The <code>commit_metrics</code> call add the parquet file in the version control framework. The metrics written in the parquet file can be retrieved using the <code>read_metrics</code> call.</p> Example <pre><code># Can be called at every epoch or every step in the training. This is logged to a parquet file and committed\n# at the commit stage.\n# Inside training loop\nwhile True:\n     cmf.log_metric(\"training_metrics\", {\"train_loss\": train_loss})\ncmf.commit_metrics(\"training_metrics\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics_name</code> <code>str</code> <p>Name to identify the metrics.</p> required <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Dictionary with metrics.</p> <code>None</code> Source code in <code>cmflib/cmf.py</code> <pre><code>def log_metric(self, metrics_name: str, custom_properties: t.Optional[t.Dict] = None) -&gt; None:\n\"\"\"Stores the fine-grained (per step or per epoch) metrics to memory.\n    The metrics provided are stored in a parquet file. The `commit_metrics` call add the parquet file in the version\n    control framework. The metrics written in the parquet file can be retrieved using the `read_metrics` call.\n    Example:\n        ```python\n        # Can be called at every epoch or every step in the training. This is logged to a parquet file and committed\n        # at the commit stage.\n        # Inside training loop\n        while True:\n             cmf.log_metric(\"training_metrics\", {\"train_loss\": train_loss})\n        cmf.commit_metrics(\"training_metrics\")\n        ```\n    Args:\n        metrics_name: Name to identify the metrics.\n        custom_properties: Dictionary with metrics.\n    \"\"\"\n    if metrics_name in self.metrics:\n        key = max((self.metrics[metrics_name]).keys()) + 1\n        self.metrics[metrics_name][key] = custom_properties\n    else:\n        self.metrics[metrics_name] = {}\n        self.metrics[metrics_name][1] = custom_properties\n</code></pre>"},{"location":"api/public/cmf/#cmflib.cmf.Cmf.create_dataslice","title":"<code>create_dataslice(name)</code>","text":"<p>Creates a dataslice object. Once created, users can add data instances to this data slice with add_data method. Users are also responsible for committing data slices by calling the commit method.</p> Example <pre><code>dataslice = cmf.create_dataslice(\"slice-a\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the dataslice.</p> required <p>Returns:</p> Type Description <code>DataSlice</code> <p>Instance of a newly created DataSlice.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def create_dataslice(self, name: str) -&gt; \"Cmf.DataSlice\":\n\"\"\"Creates a dataslice object.\n    Once created, users can add data instances to this data slice with [add_data][cmflib.cmf.Cmf.DataSlice.add_data]\n    method. Users are also responsible for committing data slices by calling the\n    [commit][cmflib.cmf.Cmf.DataSlice.commit] method.\n    Example:\n        ```python\n        dataslice = cmf.create_dataslice(\"slice-a\")\n        ```\n    Args:\n        name: Name to identify the dataslice.\n\n    Returns:\n        Instance of a newly created [DataSlice][cmflib.cmf.Cmf.DataSlice].\n    \"\"\"\n    return Cmf.DataSlice(name, self)\n</code></pre>"},{"location":"api/public/dataslice/","title":"cmflib.cmf.Cmf.DataSlice","text":"<p>A data slice represents a named subset of data. It can be used to track performance of an ML model on different slices of the training or testing dataset splits. This can be useful from different perspectives, for instance, to mitigate model bias.</p> <p>Instances of data slices are not meant to be created manually by users. Instead, use Cmf.create_dataslice method.</p> Source code in <code>cmflib/cmf.py</code> <pre><code>def __init__(self, name: str, writer):\n    self.props = {} \n    self.name = name\n    self.writer = writer\n</code></pre>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.add_data","title":"<code>add_data(path, custom_properties=None)</code>","text":"<p>Add data to create the dataslice. Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file     should already be versioned.</p> Example <pre><code>dataslice.add_data(f\"data/raw_data/{j}.xml)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Name to identify the file to be added to the dataslice.</p> required <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Properties associated with this datum.</p> <code>None</code> Source code in <code>cmflib/cmf.py</code> <pre><code>def add_data(self, path: str, custom_properties: t.Optional[t.Dict] = None) -&gt; None:\n\"\"\"Add data to create the dataslice.\n    Currently supported only for file abstractions. Pre-condition - the parent folder, containing the file\n        should already be versioned.\n    Example:\n        ```python\n        dataslice.add_data(f\"data/raw_data/{j}.xml)\n        ```\n    Args:\n        path: Name to identify the file to be added to the dataslice.\n        custom_properties: Properties associated with this datum.\n    \"\"\"\n\n    self.props[path] = {}\n    #self.props[path]['hash'] = dvc_get_hash(path)\n    parent_path = path.rsplit('/', 1)[0]\n    self.data_parent = parent_path.rsplit(\"/\",1)[1]\n    if custom_properties:\n        for k, v in custom_properties.items():\n            self.props[path][k] = v\n</code></pre>"},{"location":"api/public/dataslice/#cmflib.cmf.Cmf.DataSlice.commit","title":"<code>commit(custom_properties=None)</code>","text":"<p>Commit the dataslice. The created dataslice is versioned and added to underneath data versioning software.</p> Example <p>dataslice.commit() ```</p> <p>Parameters:</p> Name Type Description Default <code>custom_properties</code> <code>t.Optional[t.Dict]</code> <p>Dictionary to store key value pairs associated with Dataslice</p> <code>None</code> <code>Example{\"mean\"</code> <p>2.5, \"median\":2.6}</p> required Source code in <code>cmflib/cmf.py</code> <pre><code>def commit(self, custom_properties: t.Optional[t.Dict] = None) -&gt; None:\n\"\"\"Commit the dataslice.\n    The created dataslice is versioned and added to underneath data versioning software.\n    Example:\n\n        dataslice.commit()\n        ```\n    Args:\n        custom_properties: Dictionary to store key value pairs associated with Dataslice\n        Example{\"mean\":2.5, \"median\":2.6}\n    \"\"\"\n    custom_props = {} if custom_properties is None else custom_properties\n    git_repo = git_get_repo()\n    dataslice_df = pd.DataFrame.from_dict(self.props, orient='index')\n    dataslice_df.index.names = ['Path']\n    dataslice_df.to_parquet(self.name)\n    existing_artifact = []\n\n    commit_output(self.name, self.writer.execution.id)\n    c_hash = dvc_get_hash(self.name)\n    dataslice_commit = c_hash\n    remote = dvc_get_url(self.name)\n    if c_hash and c_hash.strip():\n        existing_artifact.extend(\n            self.writer.store.get_artifacts_by_uri(c_hash))\n    if existing_artifact and len(existing_artifact) != 0:\n        print(\"Adding to existing data slice\")\n        slice = link_execution_to_input_artifact(\n            store=self.writer.store,\n            execution_id=self.writer.execution.id,\n            uri=c_hash,\n            input_name=self.name + \":\" + c_hash)\n    else:\n        props = {\n            \"Commit\": dataslice_commit,\t\t\t#passing c_hash value to commit\n            \"git_repo\": git_repo,\n            \"Remote\": remote}\n        props.update(custom_props) \n        slice = create_new_artifact_event_and_attribution(\n            store=self.writer.store,\n            execution_id=self.writer.execution.id,\n            context_id=self.writer.child_context.id,\n            uri=c_hash,\n            name=self.name + \":\" + c_hash,\n            type_name=\"Dataslice\",\n            event_type=mlpb.Event.Type.OUTPUT,\n            custom_properties=props,\n            milliseconds_since_epoch=int(time.time() * 1000),\n        )\n    if self.writer.graph:\n        self.writer.driver.create_dataslice_node(\n        self.name,\n        self.name + \":\" + c_hash,\n        c_hash,\n        self.data_parent,\n        props)\n    return slice\n</code></pre>"},{"location":"architecture/advantages/","title":"Advantages","text":"<ol> <li>Tracking of metadata for distributed pipeline, thereby enabling efficient pipeline. </li> <li>Enables tracking of code, data and metadata in a single framework. </li> <li>Provides a git like ease of management for metadata.</li> <li>Provides collaboration across teams.</li> </ol>"},{"location":"architecture/components/","title":"CMF Components","text":"<p>Common metadata framework has the following components:</p> <ul> <li>Metadata Library exposes API\u2019s to track the pipeline metadata. It also provides API\u2019s to query    the stored metadata. </li> <li>Local Client interacts with the server to pull or push metadata from or to the remote store. </li> <li>Central Server interacts with all the remote clients and is responsible to merge the metadata   transferred by the remote client and manage the consolidated metadata.  </li> <li>Central Repositories hosts the code, data and metadata.</li> </ul> <p></p>"},{"location":"architecture/components/#metadata-library","title":"Metadata Library","text":"<p>The API\u2019s and the abstractions provided by the library enables tracking of pipeline metadata. It tracks the stages in  the pipeline, the input and output artifacts at each stage and metrics. The framework allows metrics to be tracked both  at coarse and fine-grained intervals. It could be a stage metrics, which could be captured at the end of a stage or  fine-grained metrics which is tracked per step (epoch) or at regular intervals during the execution of the stage. </p> <p>The metadata logged through the APIs are written to a backend relational database. The library also provides API\u2019s to  query the metadata stored in the relational database for the users to inspect pipelines.   </p> <p>In addition to explicit tracking through the API\u2019s library also provides, implicit tracking. The implicit tracking  automatically tracks the software version used in the pipelines. The function arguments and function return values can  be automatically tracked by adding metadata tracker class decorators on the functions. </p> <p>Before writing the metadata to relational database, the metadata operations are journaled in the metadata journal log.  This enables the framework to transfer the local metadata to the central server. </p> <p>All artifacts are versioned with a data versioning framework (for e.g., DVC). The content hash of the artifacts are  generated and stored along with the user provided metadata. A special artifact metadata file called a \u201c.dvc\u201d file is  created for every artifact (file / folder) which is added to data version management system. The .dvc file contains the content hash of the artifact.  </p> <p>For every new execution, the metadata tracker creates a new branch to track the code. The special metadata file created  for artifacts, the \u201c.dvc\u201d file is also committed to GIT and its commit id is tracked as a metadata information. The artifacts are versioned through the versioning of its metadata file. Whenever there is a change in the artifact,  the metadata file is modified to reflect its current content hash, and the file is tracked as a new version of the metadata file.  </p> <p>The metadata tracker automatically tracks the start commit when the library was initialized and creates separate commit  for each change in the artifact along the experiment. This helps to track the transformations on the artifacts along the different stages in the pipeline. </p>"},{"location":"architecture/components/#local-client","title":"Local Client","text":"<p>The metadata client interacts with the metadata server. It communicates with the server, for synchronization of metadata.  </p> <p>After the experiment is completed, the user invokes the \u201cCmf push\u201d command to push the collected metadata to the remote. This transfers the existing metadata journal to the server.  </p> <p>The metadata from the central repository can be pulled to the local repository, either using the artifacts or using the  project as the identifier or both. </p> <p>When artifact is used as the identifier, all metadata associated with the artifacts currently present in the branch of  the cloned Git repository is pulled from the central repository to the local repository. The pulled metadata consist of  not only the immediate metadata associated with the artifacts, it contains the metadata of all the artifacts in its  chain of lineage. </p> <p>When project is used as the identifier, all the metadata associated with the current branch of the pipeline code that  is checked out is pulled to the local repository. </p>"},{"location":"architecture/components/#central-server","title":"Central Server","text":"<p>The central server, exposes REST API\u2019s that can be called from the remote clients. This can help in situations where the connectivity between the core datacenter and the remote client is robust. The remote client calls the API\u2019s exposed by the central server to log the metadata directly to the central metadata repository.  </p> <p>Where the connectivity with the central server is intermittent, the remote clients log the metadata to the local  repository. The journaled metadata is pushed by the remote client to the central server. The central server, will  replay the journal and merge the incoming metadata with the metadata already existing in the central repository. The  ability to accurately identify the artifacts anywhere using their content hash, makes this merge robust. </p>"},{"location":"architecture/components/#central-repositories","title":"Central Repositories","text":"<p>The common metadata framework consist of three central repositories for the code, data and metadata. </p>"},{"location":"architecture/components/#central-metadata-repository","title":"Central Metadata repository","text":"<p>Central metadata repository holds the metadata pushed from the distributed sites. It holds metadata about all the  different pipelines that was tracked using the common metadata tracker.  The consolidated view of the metadata stored  in the central repository, helps the users to learn across various stages in the pipeline executed at different  locations. Using the query layer that is pointed to the central repository, the users gets the global view of the  metadata which provides them with a deeper understanding of the pipelines and its metadata.  The metadata helps to  understand nonobvious results like performance of a dataset with respect to other datasets, Performance of a particular pipeline with respect to other pipelines etc. </p>"},{"location":"architecture/components/#central-artifact-storage-repository","title":"Central Artifact storage repository","text":"<p>Central Artifact storage repository stores all the artifacts related to experiment. The data versioning framework (DVC)  stores the artifacts in a content addressable layout. The artifacts are stored inside the folder with name as the first two characters of the content hash and the name of the artifact as the remaining part of the content hash. This helps  in efficient retrieval of the artifacts.   </p>"},{"location":"architecture/components/#git-repository","title":"Git Repository","text":"<p>Git repository is used to track the code. Along with the code, the metadata file of the artifacts which contain the  content hash of the artifacts are also stored in GIT. The Data versioning framework (dvc) would use these files to  retrieve the artifacts from the artifact storage repository.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Interactions in data pipelines can be complex. The Different stages in the pipeline, (which may not be next to each  other) may have to interact to produce or transform artifacts. As the artifacts navigates and undergo transformations  through this pipeline, it can take a complicated path, which might also involve bidirectional movement across these  stages. Also, there could be dependencies between the multiple stages, where the metrics produced by a stage could influence the metrics at a subsequent stage.  It is important to track the metadata across a pipeline to provide features like,  lineage tracking, provenance and reproducibility.  </p> <p>The tracking of metadata through these complex pipelines have multiple challenges, some of them being,  </p> <ul> <li>Each stage in the pipeline could be executed in a different datacenter or an edge site having intermittent connection    to the core datacenter.   </li> <li>Each stage in the pipeline could be possibly managed by different teams.  </li> <li>The artifacts (input or output) needs to be uniquely identified across different sites and across multiple pipelines. </li> </ul> <p>Common metadata framework (CMF) addresses the problems associated with tracking of pipeline metadata from distributed  sites and tracks code, data and metadata together for end-to-end traceability.   </p> <p>The framework automatically tracks the code version as one of the metadata for an execution. Additionally, the data  artifacts are also versioned automatically using a data versioning framework (like DVC) and the metadata regarding the data version is stored along with the code. The framework stores the Git commit id of the metadata file associated with  the artifact and content hash of the artifact as metadata. The framework provides API\u2019s to track the hyperparameters and other metadata of pipelines.  Therefore, from the metadata stored, users can zero in on the hyperparameters, code  version and the artifact version used for the experiment. </p> <p>Identifying the artifacts by content hash allows the framework, to uniquely identify an artifact anywhere in the  distributed sites. This enables the metadata from the distributed sites to be precisely merged to a central repository, thereby providing a single global metadata from the distributed sites.   </p> <p>On this backbone, we build the Git like experience for metadata, enabling users to push their local metadata to the  remote repository, where it is merged to create the global metadata and pull metadata from the global metadata to the  local, to create a local view, which would contain only the metadata of interest. </p> <p>The framework can be used to track various types of pipelines such as data pipelines or AI pipelines. </p>"},{"location":"cmf_client/cmf_client/","title":"Getting started with cmf-client commands","text":""},{"location":"cmf_client/cmf_client/#cmf-init","title":"cmf init","text":"<pre>\nUsage: cmf init [-h] {minioS3,amazonS3,local,sshremote,show}\n</pre> <p>cmf init initializes an artifact repository for CMF. Local directory, Minio S3 bucket, Amazon S3 bucket and SSH Remote directory are the options available. Additionally, user can provide cmf-server IP.</p>"},{"location":"cmf_client/cmf_client/#cmf-init-show","title":"cmf init show","text":"<pre>\nUsage: cmf init show\n</pre> <p>cmf init show displays current cmf configuration.</p>"},{"location":"cmf_client/cmf_client/#cmf-init-minios3","title":"cmf init minioS3","text":"<pre>\nUsage: cmf init minioS3 [-h] --url [url] --endpoint-url [endpoint_url]\n                        --access-key-id [access_key_id] --secret-key [secret_key] --git-remote-url[git_remote_url]  --cmf-server-ip [cmf_server_ip]\n</pre> <p>cmf init minioS3 configures Minio S3 bucket as a CMF artifact repository. Refer minio-server.md to set up a minio server.</p> <p><pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://121.0.0.1:80\n</code></pre> Required Arguments</p> <pre>\n  --url [url]                           Specify bucket url.\n  --endpoint-url [endpoint_url]         Specify the endpoint url of minio UI.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --git-remote-url [git_remote_url]     Specify git repo url.\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help  show this help message and exit\n  --cmf-server-ip [cmf_server_ip]   Specify cmf-server IP. (default: http://127.0.0.1:80)\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-init-local","title":"cmf init local","text":"<pre>\nUsage: cmf init local [-h] --path [path] --git-remote-url [git_remote_url] --cmf-server-ip [cmf_server_ip]\n</pre> <p>cmf init local initialises local directory as a CMF artifact repository.</p> <pre><code>cmf init local --path /home/user/local-storage --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://121.0.0.1:80\n</code></pre> <p>Required Arguments</p> <pre>\n  --path [path]                         Specify local directory path.\n  --git-remote-url [git_remote_url]     Specify git repo url.\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help                        show this help message and exit\n  --cmf-server-ip [cmf_server_ip]   Specify cmf-server IP. (default: http://127.0.0.1:80)\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-init-amazons3","title":"cmf init amazonS3","text":"<pre>\nUsage: cmf init amazonS3 [-h] --url [url] --access-key-id [access_key_id]\n                         --secret-key [secret_key] --git-remote-url [git_remote_url] --cmf-server-ip [cmf_server_ip]\n</pre> <p>cmf init amazonS3 initialises Amazon S3 bucket as a CMF artifact repository.</p> <pre><code>cmf init amazonS3 --url s3://bucket-name --access-key-id XXXXXXXXXXXXX --secret-key XXXXXXXXXXXXX --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://121.0.0.1:80\n</code></pre> <p>Required Arguments</p> <pre>\n  --url [url]                           Specify bucket url.\n  --access-key-id [access_key_id]       Specify Access Key Id.\n  --secret-key [secret_key]             Specify Secret Key.\n  --git-remote-url [git_remote_url]     Specify git repo url.\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help  show this help message and exit\n  --cmf-server-ip [cmf_server_ip]   Specify cmf-server IP. (default: http://127.0.0.1:80)\n\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-init-sshremote","title":"cmf init sshremote","text":"<pre>\nUsage: cmf init sshremote [-h] --path [path] --user [user] --port [port]\n                          --password [password]  --git-remote-url [git_remote_url] --cmf-server-ip [cmf_server_ip]\n</pre> <p>cmf init sshremote command initialises remote ssh directory as a CMF artifact repository.</p> <pre><code>cmf init sshremote --path ssh://127.0.0.1/home/user/ssh-storage --user XXXXX --port 22 --password example@123 --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://121.0.0.1:80\n</code></pre> <p>Required Arguments</p> <pre>\n  --path [path]                           Specify ssh directory path.\n  --user [user]                           Specify user.\n  --port [port]                           Specify Port.\n  --password [password]                   Specify password.\n  --git-remote-url [git_remote_url]       Specify git repo url.\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help  show this help message and exit\n  --cmf-server-ip [cmf_server_ip]   Specify cmf-server IP. (default: http://127.0.0.1:80)\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-artifact","title":"cmf artifact","text":"<pre>\nUsage: cmf artifact [-h] {pull,push}\n</pre> <p>cmf artifact pull or push artifacts from or to the user configured artifact repository, respectively.</p>"},{"location":"cmf_client/cmf_client/#cmf-artifact-pull","title":"cmf artifact pull","text":"<pre>\nUsage: cmf artifact pull [-h] -p [pipeline_name] -f [file_name]\n</pre> <p>cmf artifact pull command pull artifacts from the user configured repository to the user's local machine. <pre><code>cmf artifact pull -p 'Test-env'  \n</code></pre></p> <p>Required Arguments</p> <pre>\n  -p [pipeline_name], --pipeline-name [pipeline_name]   Specify Pipeline name.\n\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help                                  show this help message and exit\n  -f [file_name],--file-name [file_name]      Specify mlmd file name.\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-artifact-push","title":"cmf artifact push","text":"<pre>\nUsage: cmf artifact push [-h] -p [pipeline_name] -f [file_name]\n</pre> <p>cmf artifact push command push artifacts from the user's local machine to the user configured artifact repository. <pre><code>cmf artifact push  \n</code></pre></p>"},{"location":"cmf_client/cmf_client/#cmf-metadata","title":"cmf metadata","text":"<pre>\nUsage: cmf metadata [-h] {pull,push}\n</pre> <p>cmf metadata push or pull the metadata file to and from the cmf-server, respectively.</p>"},{"location":"cmf_client/cmf_client/#cmf-metadata-pull","title":"cmf metadata pull","text":"<pre>\nUsage: cmf metadata pull [-h] -p [pipeline_name] -f [file_path]  -e [exec_id]\n</pre> <p>cmf metadata pull command pulls the metadata file from the cmf-server to the user's local machine. <pre><code>cmf metadata pull -p 'Test-env' -f \"/home/user/example/name_of_file\"\n</code></pre></p> <p>Required Arguments</p> <pre>\n  -p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n  -f [file_path], --file_path [file_path]                 Specify a location for mlmd file.\n\n</pre> <p>Optional Arguments</p> <pre>\n-h, --help                                  show this help message and exit\n-e [exec_id], --execution [exec_name]       Specify execution id\n\n</pre>"},{"location":"cmf_client/cmf_client/#cmf-metadata-push","title":"cmf metadata push","text":"<pre>\nUsage: cmf metadata push [-h] -p [pipeline_name] -f [file_name]  -e [exec_id]\n</pre> <p>cmf metadata push command pushes the metadata file from the local machine to the cmf-server.</p> <pre><code>cmf metadata push -p 'Test-env' -f \"/home/user/example/name_of_file\"\n</code></pre> <p>Required Arguments</p> <pre>\n-p [pipeline_name], --pipeline_name [pipeline_name]     Specify Pipeline name.\n</pre> <p>Optional Arguments</p> <pre>\n  -h, --help                                    show this help message and exit\n  -f [file_name], --file_name [file_name]       Specify mlmd file name.\n  -e [exec_name], --execution [exec_name]       Specify execution id.\n</pre>"},{"location":"cmf_client/minio-server/","title":"dvc minio backend","text":""},{"location":"cmf_client/minio-server/#steps-to-set-up-a-minio-server","title":"Steps to set up a minio server","text":"<p>Object storage is an abstraction layer above the file system and helps to work with data using API. Minio is the fastest way to start working with object storage. It is compatible with S3, easy to deploy, manage locally, and upscale if needed.</p> <p>Follow the below mentioned steps to set up a minio server: 1. Copy contents of the <code>example-get-started</code> directory to a separate directory outside the cmf repository.</p> <ol> <li>Check whether cmf is initialized.</li> </ol> <pre><code>cmf init show\n</code></pre> <p>If cmf is not initialized, the following message will appear on the screen.</p> <pre>\n'cmf' is not configured.\nExecute the 'cmf init' command.\n</pre> <ol> <li> <p>Execute the following command to initialize the minio S3 bucket as a CMF artifact repository. <pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git\n</code></pre></p> </li> <li> <p>Execute <code>cmf init show</code> to check the CMF configuration. The sample output looks as follows:</p> </li> </ol> <pre>\nremote.minio.url=s3://bucket-name\nremote.minio.endpointurl=http://localhost:9000\nremote.minio.access_key_id=minioadmin\nremote.minio.secret_access_key=minioadmin\ncore.remote=minio\n</pre> <ol> <li>Build a minio server using a Docker container. <code>docker-compose.yml</code> available in <code>example-get-started</code> directory provides two services: <code>minio</code> and <code>aws-cli</code>.    User will initialise the repository with bucket name, storage URL, and credentials to access minio.</li> <li> <p>Execute the following command to start the docker container. Following command requires root privileges. <pre><code>docker-compose up\n</code></pre> or <pre><code>docker compose up\n</code></pre> After executing the above command, following messages confirm that minio is up and running.</p> </li> <li> <p>Login into <code>remote.minio.endpointurl</code> (in the above example - http://localhost:9000) using access-key and secret-key mentioned in cmf configuration.</p> </li> <li> <p>Following image is an example snapshot of the minio server with bucket named 'dvc-art'.</p> </li> </ol> <p></p>"},{"location":"cmf_client/step-by-step/","title":"Getting started with cmf","text":"<p>Common metadata framework (cmf) has the following components: - Metadata Library exposes API\u2019s to track the pipeline metadata. It also provides API\u2019s to query the stored metadata.  - cmf-client interacts with the server to pull or push metadata from or to the cmf-server. - cmf-server interacts with all the remote clients and is responsible to merge the metadata transferred by the cmf-client and manage the consolidated metadata.  - Central Artifact Repositories hosts the code and data. </p>"},{"location":"cmf_client/step-by-step/#cmf-client","title":"cmf-client","text":"<p>cmf-client is a tool that facilitates metadata collaboration between different teams or two team members. It allows users to pull or push metadata from or to the cmf-server.</p>"},{"location":"cmf_client/step-by-step/#setup-a-cmf-client","title":"Setup a cmf-client","text":"<p>This section shows end-to-end setup of cmf-client.</p>"},{"location":"cmf_client/step-by-step/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>Python 3.8+</li> <li>Git latest version</li> </ul>"},{"location":"cmf_client/step-by-step/#install-cmf-library-ie-cmflib","title":"Install cmf library i.e. cmflib","text":"<p><pre><code>pip install https://github.com/HewlettPackard/cmf\n</code></pre> OR <pre><code>pip install cmflib\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#install-cmf-server","title":"Install cmf-server","text":"<p>cmf-server is a key interface for the user to explore and track their ML training runs. It allows users to store the metadata file on the cmf-server. The user can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the cmf-server. Follow here for details on how to setup a cmf-server.</p>"},{"location":"cmf_client/step-by-step/#how-to-effectively-use-cmf-client","title":"How to effectively use cmf-client?","text":"<p>Let's assume we are tracking the metadata for a pipeline named <code>Test-env</code> with minio S3 bucket as the artifact repository and a cmf-server.</p>"},{"location":"cmf_client/step-by-step/#create-a-folder","title":"Create a folder","text":"<pre><code>mkdir example-folder\n</code></pre>"},{"location":"cmf_client/step-by-step/#initialize-cmf","title":"Initialize cmf","text":"<p>CMF initialization is the first and foremost to use cmf-client commads. This command in one go complete initialization process making cmf-client user friendly. Execute cmf init in the <code>example-folder</code> directory created in the above step. <pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://x.x.x.x:8080\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#check-status-of-cmf-initialization-optional","title":"Check status of CMF initialization (Optional)","text":"<p><pre><code>cmf init show\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#track-metadata-using-cmflib","title":"Track metadata using cmflib","text":"<p>Use Sample projects as a reference to create a new project to track metadata for ML pipelines. More info is available here.</p>"},{"location":"cmf_client/step-by-step/#push-artifacts","title":"Push artifacts","text":"<p>Push artifacts in the artifact repo initialised in the Initialize cmf step. <pre><code>cmf artifact push \n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#push-metadata-to-cmf-server","title":"Push metadata to cmf-server","text":"<p><pre><code>cmf metadata push -p 'Test-env'\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#cmf-client-with-collaborative-development","title":"cmf-client with collaborative development","text":"<p>In the case of collaborative development, in addition to the above commands, users can follow the commands below to pull metadata and artifacts from a common cmf server and a central artifact repository.</p>"},{"location":"cmf_client/step-by-step/#pull-metadata-from-the-server","title":"Pull metadata from the server","text":"<p>Execute cmf metadata command in the <code>example_folder</code>. <pre><code>cmf metadata pull -p 'Test-env'\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#pull-artifacts-from-the-central-artifact-repo","title":"Pull artifacts from the central artifact repo","text":"<p>Execute cmf artifact command in the <code>example_folder</code>. <pre><code>cmf artifact pull -p \"Test-env\"\n</code></pre> Check here for more details.</p>"},{"location":"cmf_client/step-by-step/#flow-chart-for-cmf","title":"Flow Chart for cmf","text":""},{"location":"cmf_server/cmf-server/","title":"cmf-server","text":"<p>cmf-server is a key interface for the user to explore and track their ML training runs. It allows users to store the metadata file on the cmf-server. The user can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the cmf-server.</p>"},{"location":"cmf_server/cmf-server/#api-reference","title":"API Reference","text":"<p>cmf-server APIs are organized around FastAPI. They accept and return JSON-encoded request bodies and responses and return standard HTTP response codes.</p>"},{"location":"cmf_server/cmf-server/#list-of-apis","title":"List of APIs","text":"Method URL Description <code>Post</code> <code>/mlmd_push</code> Used to push Json Encoded data to cmf-server <code>Get</code> <code>/mlmd_pull/{pipeline_name}</code> Retrieves a mlmd file from cmf-server <code>Get</code> <code>/display_executions</code> Retrieves all executions from cmf-server <code>Get</code> <code>/display_artifacts</code> Retrieves all artifacts from cmf-server"},{"location":"cmf_server/cmf-server/#http-response-status-codes","title":"HTTP Response Status codes","text":"Code Title Description <code>200</code> <code>OK</code> mlmd is successfully pushed (e.g. when using <code>GET</code>, <code>POST</code>). <code>400</code> <code>Bad request</code> When the cmfenv-server is not available. <code>500</code> <code>Internal server error</code> When an internal error has happened"},{"location":"cmf_server/cmf-server/#setup-a-cmf-server","title":"Setup a cmf-server","text":""},{"location":"cmf_server/cmf-server/#pre-requisite","title":"Pre-requisite","text":"<ol> <li>Install Docker with non root user privileges.</li> </ol> <p>### Following steps start a cmf-server in a docker conatainer: 1.  Install cmflib on your system.</p> <ol> <li> <p>Go to server directory.  <pre><code>cd server\n</code></pre></p> </li> <li> <p>List all docker images. <pre><code>docker images\n</code></pre></p> </li> <li> <p>Execute the below-mentioned command to create a cmf-server docker image.</p> </li> </ol> <pre>\nUsage:  docker build -t [image_name] -f ./Dockerfile ../\n</pre> <p>Example: <pre><code>docker build -t myimage -f ./Dockerfile ../\n</code></pre> Note - <code>'../'</code>  represents the Build context for the docker image.</p> <ol> <li>Launch a new docker container using the image with volume created in the previous step </li> </ol> <pre>\nUsage: docker run --name [container_name] --port 8080:80 -v /home/user/cmf-server/data:/cmf-server/data [image_name]\n</pre> <p>Example: <pre><code>docker run --name mycontainer -p 8080:80 -v /home/user/cmf-server/data:/cmf-server/data myimage\n</code></pre> 6. To stop the docker container. <pre><code>docker stop [container_name]\n</code></pre></p> <ol> <li> <p>To delete the docker container. <pre><code>docker rm [container_name] \n</code></pre></p> </li> <li> <p>To remove the docker image. <pre><code>docker image rm [image_name] \n</code></pre></p> </li> </ol>"},{"location":"examples/getting_started/","title":"Getting Started","text":"<p>This example depends on the following packages: <code>git</code>. We also recommend installing  anaconda to manage python virtual environments. This example was tested in the following environments: </p> <ul> <li><code>Ubuntu-22.04 with python-3.8.15</code></li> </ul> <p>This example demonstrates how CMF tracks a metadata associated with executions of various machine learning (ML)  pipelines. ML pipelines differ from other pipelines (e.g., data Extract-Transform-Load pipelines) by the presence of ML steps, such as training and testing ML models. More comprehensive ML pipelines may include steps such as deploying a trained model and tracking its inference parameters (such as response latency, memory consumption etc.). This example,  located here implements a simple pipeline consisting of four steps:</p> <ul> <li>The parse step splits   the raw data into    <code>train</code> and <code>test</code> raw datasets for training and testing a machine learning model. This step registers one   input artifact (raw <code>dataset</code>) and two output artifacts (train and test <code>datasets</code>). </li> <li>The featurize   step creates two machine learning splits - train and test splits - that will be used by an ML training algorithm to   train ML models. This step registers two input artifacts (raw train and test datasets) and two output artifacts (   train and test ML datasets). </li> <li>The next train step   trains an ML model (random forest classifier). It registers one input artifact (train ML dataset) and one   output artifact (trained ML model).</li> <li>The fourth test step   tests the ML model trained in the third <code>train</code> step. This step registers two input artifacts (ML model and test   dataset) and one output artifact (performance metrics).</li> <li>The last query step   is a demonstration that shows how pipeline metadata can be retrieved from CMF. It will print metadata associated with   all executions of the above steps. This means that if you rerun the pipeline again, the output will include not only   metadata associated with the last run, but also metadata associated with all previous runs.</li> </ul>"},{"location":"examples/getting_started/#pre-requisites","title":"Pre-requisites","text":"<p>We start by creating (1) a workspace directory that will contain all files for this example and (2) a python virtual  environment. Then we will clone the CMF project that contains this example project. <pre><code># Create workspace directory\nmkdir cmf_getting_started_example\ncd cmf_getting_started_example\n\n# Create and activate Python virtual environment (the Python version may need to be adjusted depending on your system)\nconda create -n cmf_getting_started_example python=3.8 \nconda activate cmf_getting_started_example\n\n# Clone the CMF project from GitHub and install CMF\ngit clone https://github.com/HewlettPackard/cmf\npip install ./cmf\n</code></pre></p>"},{"location":"examples/getting_started/#setup-a-cmf-server","title":"Setup a cmf-server","text":"<p>cmf-server is a key interface for the user to explore and track their ML training runs. It allows users to store the metadata file on the cmf-server. The user can retrieve the saved metadata file and can view the content of the saved metadata file using the UI provided by the cmf-server.</p> <p>Follow here to setup a common cmf-server.</p>"},{"location":"examples/getting_started/#project-initialization","title":"Project initialization","text":"<p>We need to copy the source tree of the example in its own directory (that must be outside the CMF source tree), and using <code>cmf init</code> command initialize dvc remote directory, git remote url and cmf server with appropriate dvc backend for this project .</p> <pre><code># Create a separate copy of the example project\ncp -r ./cmf/examples/example-get-started/ ./example-get-started\ncd ./example-get-started\n</code></pre>"},{"location":"examples/getting_started/#cmf-init","title":"cmf init","text":"<pre>\nUsage: cmf init minioS3 [-h] --url [url] --endpoint-url [endpoint_url]\n                        --access-key-id [access_key_id] --secret-key [secret_key] --git-remote-url[git_remote_url]  --cmf-server-ip [cmf_server_ip]\n</pre> <p><pre><code>cmf init minioS3 --url s3://bucket-name --endpoint-url http://localhost:9000 --access-key-id minioadmin --secret-key minioadmin --git-remote-url https://github.com/user/experiment-repo.git --cmf-server-ip http://121.0.0.1:80\n</code></pre> Follow here for more details.</p>"},{"location":"examples/getting_started/#project-execution","title":"Project execution","text":"<p>To execute the example pipeline, run the  test_script.sh  file (before that, study the contents of that file). Basically, that script will run a sequence of steps common for a typical machine learning project - getting raw data, converting it into machine learning train/test splits, training and testing a model. The execution of these steps (and parent pipeline) will be recorded by the CMF. <pre><code># Run the example pipeline\nsh ./test_script.sh\n</code></pre></p> <p>This script will run the pipeline and will store its metadata in a sqlite file named mlmd. Verify that all stages are  done using <code>git log</code> command. You should see commits corresponding to the artifacts that were created.</p> <p>Under normal conditions, the next steps would be to: (1) execute the <code>cmf artifact push</code> command to push the artifacts to the central artifact repository and (2) execute the <code>cmf metadata push</code> command to track the metadata of the generated artifacts on a common cmf server.</p> <p>Follow here for more details on <code>cmf artifact</code> and <code>cmf metadata</code> commands.</p>"},{"location":"examples/getting_started/#query","title":"Query","text":"<p>The stored metadata can be explored using the query layer. Example Jupyter notebook  Query_Tester-base_mlmd.ipynb can be found in this directory.</p>"},{"location":"examples/getting_started/#clean-up","title":"Clean Up","text":"<p>Metadata is stored in sqlite file named \"mlmd\". To clean up, delete the \"mlmd\" file.</p>"},{"location":"examples/getting_started/#steps-to-test-dataslice","title":"Steps to test dataslice","text":"<p>Run the following command: <code>python test-data-slice.py</code>.</p>"}]}